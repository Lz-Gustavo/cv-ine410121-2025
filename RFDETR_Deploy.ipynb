{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12452b48-d8cc-4f7a-8b6a-c05df1cd6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from rfdetr import RFDETRSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "435b3968-b3d1-4d9a-837f-d05b597c01d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: RFDETRSmall_Mono.pth\n",
      "Using a different number of positional encodings than DINOv2, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.\n",
      "Using patch size 16 instead of 14, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_classes mismatch: pretrain weights has 0 classes, but your model has 90 classes\n",
      "reinitializing detection head with 0 classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrain weights\n",
      "Model loaded and optimized\n",
      "Starting webcam (ID: 0). Press 'q' to quit.\n",
      "Webcam stream closed\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "MODE = \"webcam\"  # Options: \"video\", \"webcam\"\n",
    "MODEL_PATH = \"RFDETRSmall_Mono.pth\"  # Specific .pth path\n",
    "CLASS_DICT = \"monolabel\"  # Options: \"monolabel\", \"multilabel\"\n",
    "\n",
    "# Input/Output paths (for video mode)\n",
    "INPUT_VIDEO = \"salgadinos.mp4\"\n",
    "OUTPUT_VIDEO = \"salgadinos_mono.mp4\"\n",
    "\n",
    "# Webcam settings\n",
    "WEBCAM_ID = 0  # Default webcam\n",
    "\n",
    "# Detection threshold\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# ========== CLASS DICTIONARIES ==========\n",
    "CLASS_DICTS = {\n",
    "    \"monolabel\": {\n",
    "        0: \"Salgado\"\n",
    "    },\n",
    "    \"multilabel\": {\n",
    "        0: \"Bolinha de queijo\",\n",
    "        1: \"Canapé\",\n",
    "        2: \"Canudo\",\n",
    "        3: \"Coxinha\",\n",
    "        4: \"Croquete\",\n",
    "        5: \"Empadinha\",\n",
    "        6: \"Enroladinho de salsicha\",\n",
    "        7: \"Esfiha\",\n",
    "        8: \"Folhado\",\n",
    "        9: \"Pastelzinho\",\n",
    "        10: \"Pão de queijo\",\n",
    "        11: \"Quibe\",\n",
    "        12: \"Risoles\",\n",
    "        13: \"Sanduiche\",\n",
    "    }\n",
    "}\n",
    "\n",
    "COCO_CLASSES = CLASS_DICTS[CLASS_DICT]\n",
    "\n",
    "# ========== LOAD MODEL ==========\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "model = RFDETRSmall(pretrain_weights=MODEL_PATH)\n",
    "model.optimize_for_inference()\n",
    "print(\"Model loaded and optimized\")\n",
    "\n",
    "# ========== SETUP ANNOTATORS ==========\n",
    "color = sv.ColorPalette.from_hex([\n",
    "    \"#ffff00\", \"#ff9b00\", \"#ff8080\", \"#ff66b2\", \"#ff66ff\", \"#b266ff\",\n",
    "    \"#9999ff\", \"#3399ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
    "])\n",
    "\n",
    "def annotate_frame(frame, detections, counts_text):\n",
    "    \"\"\"Annotate a single frame with detections and counts\"\"\"\n",
    "    # Convert BGR to RGB for PIL\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    text_scale = sv.calculate_optimal_text_scale(resolution_wh=image.size)\n",
    "    thickness = sv.calculate_optimal_line_thickness(resolution_wh=image.size)\n",
    "    \n",
    "    bbox_annotator = sv.BoxAnnotator(color=color, thickness=thickness)\n",
    "    label_annotator = sv.LabelAnnotator(\n",
    "        color=color,\n",
    "        text_color=sv.Color.BLACK,\n",
    "        text_scale=text_scale,\n",
    "        smart_position=True\n",
    "    )\n",
    "    \n",
    "    labels = [\n",
    "        f\"{COCO_CLASSES[class_id]} {confidence:.2f}\"\n",
    "        for class_id, confidence\n",
    "        in zip(detections.class_id, detections.confidence)\n",
    "    ]\n",
    "    \n",
    "    annotated_image = bbox_annotator.annotate(image, detections)\n",
    "    annotated_image = label_annotator.annotate(annotated_image, detections, labels)\n",
    "    \n",
    "    # Add count overlay\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"VeraMono.ttf\", 40)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    draw.multiline_text((10, 10), counts_text, fill=\"red\", font=font)\n",
    "    \n",
    "    # Convert back to BGR for OpenCV\n",
    "    return cv2.cvtColor(np.array(annotated_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def process_frame(frame):\n",
    "    \"\"\"Process a single frame and return annotated frame\"\"\"\n",
    "    # Convert to PIL Image\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Run detection\n",
    "    detections = model.predict(image, threshold=THRESHOLD)\n",
    "    \n",
    "    # Count classes\n",
    "    counts = {cid: 0 for cid in COCO_CLASSES.keys()}\n",
    "    for cid in detections.class_id:\n",
    "        counts[int(cid)] += 1\n",
    "    \n",
    "    # Build count text\n",
    "    lines = []\n",
    "    for cid in sorted(counts.keys()):\n",
    "        if counts[cid] > 0:  # Only show detected classes\n",
    "            lines.append(f\"{COCO_CLASSES[cid]}: {counts[cid]}\")\n",
    "    counts_text = \"\\n\".join(lines) if lines else \"No detections\"\n",
    "    \n",
    "    # Annotate frame\n",
    "    return annotate_frame(frame, detections, counts_text)\n",
    "\n",
    "# ========== VIDEO MODE ==========\n",
    "if MODE == \"video\":\n",
    "    print(f\"Processing video: {INPUT_VIDEO}\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(INPUT_VIDEO)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Cannot open video: {INPUT_VIDEO}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Setup video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Video info: {width}x{height} @ {fps}fps, {total_frames} frames\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        print(f\"Processing frame {frame_count}/{total_frames}\", end='\\r')\n",
    "        \n",
    "        annotated_frame = process_frame(frame)\n",
    "        out.write(annotated_frame)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"\\nVideo saved to: {OUTPUT_VIDEO}\")\n",
    "\n",
    "# ========== WEBCAM MODE ==========\n",
    "elif MODE == \"webcam\":\n",
    "    print(f\"Starting webcam (ID: {WEBCAM_ID}). Press 'q' to quit.\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(WEBCAM_ID)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Cannot open webcam: {WEBCAM_ID}\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        annotated_frame = process_frame(frame)\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow('RFDETR Webcam Detection', annotated_frame)\n",
    "        \n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Webcam stream closed\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Invalid MODE: {MODE}. Use 'video' or 'webcam'\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8dae4a-1f5d-4de5-882e-0fae31f001f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
