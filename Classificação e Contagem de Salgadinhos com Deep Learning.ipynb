{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ced7d0-bf03-43ba-932a-3cfed166cfff",
   "metadata": {},
   "source": [
    "# Classificação e contagem de Salgadinhos com Deep Learning\n",
    "Autores: Vicente Knobel Borges e Luiz Gustavo Xavier\n",
    "\n",
    "Neste relatório é apresentada a solução de um problema de visão computacional aplicado a indústria de alimentos, como requisitos para a disciplina INE410121 - Visão Computacional. O projeto consiste na aplicação de algoritmos clássicos de visão computacional e redes neurais artificiais para solução do problema de contagem de salgadinhos sortidos em caixas mistas. Em nossa melhor solução com o *fine tune* do modelo pré-treinado [RF-DETR](https://github.com/roboflow/rf-detr), foi possível identificar corretamente diversas classes de salgadinhos em fotografias diversas de caixas, obtendo YY de acurácia em nosso dataset.\n",
    "\n",
    "O processo de geração do modelo se deu por uma sequencia de passos:\n",
    "1. Busca manual de imagens exemplo no google imagens\n",
    "2. Geração de [*novel view synthesis*](https://en.wikipedia.org/wiki/View_synthesis) das imagens adquiridas usando o modelo [Nano Banana Pro](https://blog.google/technology/ai/nano-banana-pro/) da Google.\n",
    "3. *Labeling* bruto usando [sam3](https://github.com/facebookresearch/sam3)\n",
    "4. Refinamento das labels usando [Label Studio](https://labelstud.io/)\n",
    "5. Treinamento usando modelos [RF-DETR](https://github.com/roboflow/rf-detr) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c032cd-cd3f-4355-83db-19795597636a",
   "metadata": {},
   "source": [
    "## Geração do *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3d39a-27c1-49bf-92f9-36d9c1846983",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Busca manual pelo google imagens\n",
    "Começamos o processo definindo quais salgadinhos o modelo deve indentificar. Buscamos representar os salgadinhos mais comuns vendidos por empresas de alimentos para festas, chegando na lista:\n",
    "- Bolinha de queijo\n",
    "- Canapé\n",
    "- Canudo\n",
    "- Coxinha\n",
    "- Croquete\n",
    "- Empadinha\n",
    "- Enroladinho de salsicha\n",
    "- Esfiha\n",
    "- Folhado\n",
    "- Pastelzinho\n",
    "- Pão de queijo\n",
    "- Quibe\n",
    "- Risoles\n",
    "- Sanduiche\n",
    "\n",
    "Buscamos 30 imagens de cada classe, além de um conjunto de imagens de bandejas cheias, denominado 'grupo'. A Heuristica de inclusão priorizava a escolha de imagens por:\n",
    "- Conter multiplos exemplares do salgadinho buscado em uma unica foto\n",
    "- Salgadinhos fotografados no angulo *top-down*\n",
    "- Salgadinhos em bandejas são preferenciais a salgadinhos em potes ou empilhados\n",
    "\n",
    "![imagens de coxinhas selecionadas](relatorio2Images/coxinhas_google.jpg)\n",
    "Acima: Imagens de coxinhas selecionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c30516-ede7-410c-aee9-ec75a815fa25",
   "metadata": {},
   "source": [
    "### Novel view synthesis com Nano Banana Pro\n",
    "No processo de buscar exemplos no google, nos deparamos com um forte viés em fotos de uma perspectiva lateral ao corpo de salgados, coerente com propagandas e divulgação. Embora seja relevante que nosso modelo trabalhe com este outro angulo de visão, achamos preocupante a ausência de fotos de vista superior.\n",
    "\n",
    "A vista superior é a perspectiva que estipulamos como nosso caso de uso principal com o modelo sendo colocado em produção por padeiros interessados em contar os salgados em suas caixas antes de despachar para entrega.\n",
    "\n",
    "Para gerar essas vistas fizemos uso do novo modelo de geração, edição e manipulação de imagens da Google, o [Nano Banana Pro](https://blog.google/technology/ai/nano-banana-pro/).\n",
    "\n",
    "O modelo está disponivel pelo [serviço de API dos modelos Gemini](https://ai.google.dev/gemini-api/docs/image-generation) e por meio de *prompt engineering*, implementamos uma geração que extrapola quatro novas imagens a partir da imagem original, em uma unica geração.\n",
    "\n",
    "- Uma geração de vista lateral, similar a imagem original;\n",
    "- Uma geração top-down, dos salgadinhos no chão - mudando o fundo dos salgadinhos e variando o fundo;\n",
    "- Uma geração top-down, com luz noturna, diversificando condições de iluminação\n",
    "- Uma geração top-down dos salgados na sua disposição original.\n",
    "\n",
    "![imagens de coxinhas geradas pelo modelo Nano Banana](relatorio2Images/nanoBanana.jpg)\n",
    "Acima: Imagens de coxinhas geradas pelo modelo Nano Banana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ffafa3-2585-47ce-b77a-07dde7217770",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Novel View Synthesis: código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2053493f-57da-492f-abb8-c86bb59eeaa3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c09e891-3d86-4e69-b2cb-780e88508950",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U google-genai #api para acesso aos modelos de IA da Google\n",
    "!pip install -U pillow #bibilioteca de manipulação de imagens\n",
    "!pip install -U opencv-python #bibilioteca de visão computacional\n",
    "!pip install -U tqdm #progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84589b86-3e27-4827-b925-fbab9a25da24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### imports e definições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6608da14-60fb-44f5-8ae6-3a40a5610322",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key='' #your google api key here\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e48464-8d42-457f-a68c-82f3d5bebdae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d79e98b-0837-4cac-adee-773198b7c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nanoBananaGeneration(client, prompt, image_path_in, image_path_out):\n",
    "    # Define a proporção de aspecto da imagem gerada\n",
    "    aspect_ratio = \"5:4\" # \"1:1\",\"2:3\",\"3:2\",\"3:4\",\"4:3\",\"4:5\",\"5:4\",\"9:16\",\"16:9\",\"21:9\"\n",
    "    # Define a resolução da imagem gerada\n",
    "    resolution = \"1K\" # \"1K\", \"2K\", \"4K\"\n",
    "    \n",
    "    # Faz uma chamada ao modelo Gemini para gerar conteúdo (texto e imagem)\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-3-pro-image-preview\",\n",
    "        # Passa o prompt de texto e a imagem de entrada como conteúdo\n",
    "        contents=[\n",
    "            prompt,\n",
    "            Image.open(image_path_in),\n",
    "        ],\n",
    "        # Configura os parâmetros de geração\n",
    "        config=types.GenerateContentConfig(\n",
    "            # Define que a resposta pode conter texto e imagem\n",
    "            response_modalities=['TEXT', 'IMAGE'],\n",
    "            # Configura as propriedades da imagem a ser gerada\n",
    "            image_config=types.ImageConfig(\n",
    "                aspect_ratio=aspect_ratio,\n",
    "                image_size=resolution\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Itera sobre as partes da resposta gerada\n",
    "    for part in response.parts:\n",
    "        # Se a parte contém texto, imprime no console\n",
    "        if part.text is not None:\n",
    "            print(part.text)\n",
    "        # Se a parte contém uma imagem, salva no caminho de saída\n",
    "        elif image:= part.as_image():\n",
    "            image.save(image_path_out)\n",
    "\n",
    "def collect_image_paths(input_folder):\n",
    "    # Define as extensões de arquivo de imagem aceitas\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "    # Inicializa lista vazia para armazenar os caminhos\n",
    "    paths = []\n",
    "    # Percorre recursivamente todos os arquivos e subpastas da pasta de entrada\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        # Para cada arquivo encontrado\n",
    "        for f in files:\n",
    "            # Verifica se a extensão do arquivo está na lista de extensões aceitas (ignorando maiúsculas/minúsculas)\n",
    "            if Path(f).suffix.lower() in exts:\n",
    "                # Adiciona o caminho completo do arquivo à lista\n",
    "                paths.append(Path(root) / f)\n",
    "    # Retorna a lista de caminhos das imagens encontradas\n",
    "    return paths\n",
    "\n",
    "def filter_unprocessed(paths, input_folder, output_folder):\n",
    "    # Converte as pastas de entrada e saída para objetos Path\n",
    "    input_folder = Path(input_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    # Inicializa lista para armazenar apenas os caminhos não processados\n",
    "    filtered = []\n",
    "    # Para cada caminho de imagem de origem\n",
    "    for src_path in paths:\n",
    "        # Obtém o caminho relativo da imagem em relação à pasta de entrada\n",
    "        rel = src_path.relative_to(input_folder)\n",
    "        # Remove a extensão do caminho relativo\n",
    "        rel_no_ext = rel.with_suffix(\"\")\n",
    "        # Constrói o caminho de destino na pasta de saída com extensão .jpg\n",
    "        dst_path = (output_folder / rel_no_ext).with_suffix(\".jpg\")\n",
    "        # Se o arquivo de destino não existe, adiciona à lista de não processados\n",
    "        if not dst_path.exists():\n",
    "            filtered.append(src_path)\n",
    "    # Retorna apenas os caminhos que ainda não foram processados\n",
    "    return filtered\n",
    "\n",
    "def process_paths(paths, input_folder, output_folder, client, prompt):\n",
    "    # Converte as pastas de entrada e saída para objetos Path\n",
    "    input_folder = Path(input_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    \n",
    "    # Garante que a pasta de saída raiz existe (cria se não existir)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    # Itera sobre os caminhos com barra de progresso usando tqdm\n",
    "    for src_path in tqdm(paths, desc=\"Processing images\"):\n",
    "        # Obtém o caminho relativo da imagem em relação à pasta de entrada\n",
    "        rel_path = src_path.relative_to(input_folder)\n",
    "        # Remove a extensão do caminho relativo\n",
    "        rel_no_ext = rel_path.with_suffix(\"\")\n",
    "        # Constrói o caminho de destino na pasta de saída com extensão .jpg\n",
    "        dst_path = (output_folder / rel_no_ext).with_suffix(\".jpg\")\n",
    "        # Garante que as subpastas necessárias existem no destino\n",
    "        dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Tenta processar a imagem\n",
    "        try:\n",
    "            # Abre a imagem de origem e converte para RGB\n",
    "            img = Image.open(src_path).convert(\"RGB\")\n",
    "            # Chama a função de geração com o cliente, prompt e caminhos\n",
    "            nanoBananaGeneration(client, prompt, src_path, dst_path)\n",
    "        # Se houver qualquer erro, ignora e continua para a próxima imagem\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "def resize_to_box(img, box_w=1440, box_h=720):\n",
    "    # Obtém a altura e largura da imagem\n",
    "    h, w = img.shape[:2]\n",
    "    # Calcula o fator de escala para caber na caixa, mantendo proporção (usa o menor dos dois)\n",
    "    scale = min(box_w / w, box_h / h)\n",
    "    # Calcula a nova largura aplicando o fator de escala\n",
    "    new_w = int(w * scale)\n",
    "    # Calcula a nova altura aplicando o fator de escala\n",
    "    new_h = int(h * scale)\n",
    "    # Redimensiona a imagem com interpolação INTER_AREA (boa para redução)\n",
    "    return cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def create_collage(input_folder, out_path=\"collage.jpg\", box_w=1440, box_h=720):\n",
    "    # Converte a pasta de entrada para objeto Path\n",
    "    input_folder = Path(input_folder)\n",
    "    # Obtém lista ordenada de todos os caminhos de arquivos na pasta\n",
    "    image_paths = sorted(glob.glob(str(input_folder / \"*\")))\n",
    "    # Inicializa lista vazia para armazenar as imagens carregadas\n",
    "    images = []\n",
    "    # Para cada caminho de imagem\n",
    "    for p in image_paths:\n",
    "        # Tenta carregar a imagem usando OpenCV\n",
    "        img = cv2.imread(p)\n",
    "        # Se a imagem não pôde ser carregada, pula para a próxima\n",
    "        if img is None:\n",
    "            continue\n",
    "        # Redimensiona a imagem para caber na caixa definida\n",
    "        img = resize_to_box(img, box_w, box_h)\n",
    "        # Adiciona a imagem redimensionada à lista\n",
    "        images.append(img)\n",
    "    # Se nenhuma imagem foi carregada, imprime mensagem e sai\n",
    "    if not images:\n",
    "        print(\"No images found.\")\n",
    "        return\n",
    "    # --- constrói grade automaticamente ---\n",
    "    # Conta o número total de imagens\n",
    "    n = len(images)\n",
    "    # Calcula o número de colunas (arredonda para cima a raiz quadrada)\n",
    "    cols = math.ceil(math.sqrt(n))\n",
    "    # Calcula o número de linhas necessárias\n",
    "    rows = math.ceil(n / cols)\n",
    "    # Define largura e altura de cada célula da grade\n",
    "    cell_w, cell_h = box_w, box_h\n",
    "    # Calcula largura total da colagem\n",
    "    collage_w = cols * cell_w\n",
    "    # Calcula altura total da colagem\n",
    "    collage_h = rows * cell_h\n",
    "    # Cria um array numpy vazio (preto) para a colagem completa\n",
    "    collage = np.zeros((collage_h, collage_w, 3), dtype=np.uint8)\n",
    "    # Inicializa índice da imagem atual\n",
    "    i = 0\n",
    "    # Para cada linha da grade\n",
    "    for r in range(rows):\n",
    "        # Para cada coluna da grade\n",
    "        for c in range(cols):\n",
    "            # Se já processou todas as imagens, interrompe\n",
    "            if i >= n: break\n",
    "            # Pega a imagem atual\n",
    "            img = images[i]\n",
    "            # Calcula o deslocamento vertical para centralizar a imagem na célula\n",
    "            y_offset = r * cell_h + (cell_h - img.shape[0]) // 2\n",
    "            # Calcula o deslocamento horizontal para centralizar a imagem na célula\n",
    "            x_offset = c * cell_w + (cell_w - img.shape[1]) // 2\n",
    "            # Coloca a imagem na posição calculada dentro da colagem\n",
    "            collage[y_offset:y_offset+img.shape[0],\n",
    "                    x_offset:x_offset+img.shape[1]] = img\n",
    "            # Avança para a próxima imagem\n",
    "            i += 1\n",
    "\n",
    "    # Redimensiona a colagem para ter o tamanho final da caixa de interesse\n",
    "    collage = resize_to_box(collage, box_w, box_h)\n",
    "    # Salva a colagem final no caminho especificado\n",
    "    cv2.imwrite(out_path, collage)\n",
    "    # Imprime mensagem confirmando o salvamento\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "def slice_images_into_four(input_folder, output_folder=\"imagens_cortadas\"):\n",
    "    # Converte as pastas de entrada e saída para objetos Path\n",
    "    input_folder = Path(input_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    \n",
    "    # Define as extensões de arquivo de imagem aceitas\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "    \n",
    "    # Garante que a pasta de saída raiz existe (cria se não existir)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Coleta todos os caminhos de imagens recursivamente\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        for f in files:\n",
    "            if Path(f).suffix.lower() in exts:\n",
    "                image_paths.append(Path(root) / f)\n",
    "    \n",
    "    # Itera sobre os caminhos com barra de progresso usando tqdm\n",
    "    for src_path in tqdm(image_paths, desc=\"Slicing images\"):\n",
    "        # Tenta processar a imagem\n",
    "        try:\n",
    "            # Carrega a imagem usando OpenCV\n",
    "            img = cv2.imread(str(src_path))\n",
    "            # Se a imagem não pôde ser carregada, pula para a próxima\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Obtém a altura e largura da imagem\n",
    "            h, w = img.shape[:2]\n",
    "            # Calcula a altura de cada fatia (metade da altura total)\n",
    "            half_h = h // 2\n",
    "            # Calcula a largura de cada fatia (metade da largura total)\n",
    "            half_w = w // 2\n",
    "            \n",
    "            # Define as 4 fatias da imagem (superior esquerda, superior direita, inferior esquerda, inferior direita)\n",
    "            slices = [\n",
    "                img[0:half_h, 0:half_w],           # fatia 0: superior esquerda\n",
    "                img[0:half_h, half_w:w],           # fatia 1: superior direita\n",
    "                img[half_h:h, 0:half_w],           # fatia 2: inferior esquerda\n",
    "                img[half_h:h, half_w:w]            # fatia 3: inferior direita\n",
    "            ]\n",
    "            \n",
    "            # Obtém o caminho relativo da imagem em relação à pasta de entrada\n",
    "            rel_path = src_path.relative_to(input_folder)\n",
    "            # Obtém o caminho relativo sem a extensão\n",
    "            rel_no_ext = rel_path.with_suffix(\"\")\n",
    "            \n",
    "            # Para cada fatia (0 a 3)\n",
    "            for idx, slice_img in enumerate(slices):\n",
    "                # Constrói o caminho de destino adicionando o índice da fatia ao nome\n",
    "                dst_path = (output_folder / f\"{rel_no_ext}_{idx}\").with_suffix(\".jpg\")\n",
    "                # Garante que as subpastas necessárias existem no destino\n",
    "                dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                # Salva a fatia da imagem\n",
    "                cv2.imwrite(str(dst_path), slice_img)\n",
    "        \n",
    "        # Se houver qualquer erro, ignora e continua para a próxima imagem\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {src_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Imprime mensagem de conclusão\n",
    "    print(f\"Slicing complete! Images saved to: {output_folder}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0526bb-0b8d-4d66-b918-1e6a7e3cf863",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### execução\n",
    "Folder com as imagens baixadas deve estar na estrutura\n",
    "```\n",
    "dataset/\n",
    "├── coxinha/\n",
    "│   ├── coxinha0001.jpg\n",
    "│   ├── coxinha0002.jpg\n",
    "│   ├── coxinha0003.jpg\n",
    "│   └── ...\n",
    "├── bolinha_de_queijo/\n",
    "│   ├── bolinha0001.jpg\n",
    "│   ├── bolinha0002.jpg\n",
    "│   └── ...\n",
    "└── pastel/\n",
    "    ├── pastel0001.jpg\n",
    "    ├── pastel0002.jpg\n",
    "    └── ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65222b96-9103-46c5-81e5-a39fcd637443",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"dataset\"\n",
    "output_folder = \"dataset_sinth\"\n",
    "\n",
    "#coleta paths de todas as imagens\n",
    "paths = collect_image_paths(input_folder) \n",
    "\n",
    "#remove da lista imagens do qual as vistas sintéticas já foram criadas\n",
    "paths = filter_unprocessed(paths, input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd487e52-b3d8-4f51-a0e4-f03dfa0754e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprime os paths das imagens baixadas do google\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398dab4b-aebd-42f3-9eb9-e0cb336ddbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o cliente recebe a chave da API declarada acima\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "#o prompt detalha as visadas da imagem apresentada que nos interessam.\n",
    "prompt = (\"\"\"\n",
    "Hi! I'm doing a image training dataset of Brazilian Salgadinhos, and want to generate novel views of the image i sent you. Follow the generation guide below please!\n",
    " {\n",
    "\n",
    "  \"style_mode\": \"raw_photoreal_documentary_collage\",\n",
    "\n",
    "  \"look\": \"casual product photography, domestic setting, natural and ambient lighting, 2x2 grid layout\",\n",
    "\n",
    "  \"layout_structure\": {\n",
    "\n",
    "    \"format\": \"four-panel collage (2x2 grid)\",\n",
    "\n",
    "    \"description\": \"The image is divided into four distinct rectangular quadrants, each presenting a different angle and lighting condition of the subject on the source image.\"\n",
    "\n",
    "  },\n",
    "\n",
    "  \"camera\": {\n",
    "\n",
    "    \"vantage\": \"variable per quadrant (high-angle, eye-level, low-angle, top-down)\",\n",
    "\n",
    "    \"framing\": \"medium to close-up shots\",\n",
    "\n",
    "    \"lens_behavior\": \"smartphone camera aesthetic, varying depth of field (shallow in macro shots, deep in environmental shots)\",\n",
    "\n",
    "    \"sensor_quality\": \"standard digital photography, slight ISO noise in darker areas, realistic sharpness\"\n",
    "\n",
    "  },\n",
    "\n",
    "  \"scene\": {\n",
    "\n",
    "    \"quadrant_details\": {\n",
    "\n",
    "      \"top_left\": {\n",
    "\n",
    "        \"perspective\": \"high-angle three-quarter view\",\n",
    "\n",
    "        \"lighting\": \"same as the source image\",\n",
    "\n",
    "        \"background\": \"same as the source image\",\n",
    "\n",
    "      },\n",
    "\n",
    "      \"top_right\": {\n",
    "\n",
    "        \"perspective\": \"direct top-down (flat lay)\",\n",
    "\n",
    "        \"lighting\": \"same as the source image\",\n",
    "\n",
    "        \"background\": \"Salgadinhos on the image out of the tray and on the floor\",\n",
    "\n",
    "\n",
    "      },\n",
    "\n",
    "      \"bottom_left\": {\n",
    "\n",
    "        \"perspective\": \"direct top-down (flat lay)\",\n",
    "\n",
    "        \"lighting\": \"nightime lighting\",\n",
    "\n",
    "        \"background\": \"same as the source image\",\n",
    "\n",
    "      },\n",
    "\n",
    "      \"bottom_right\": {\n",
    "\n",
    "        \"perspective\": \"direct top-down (flat lay)\",\n",
    "\n",
    "        \"lighting\": \"same as the source image\",\n",
    "\n",
    "        \"background\": \"same as the source image\",\n",
    "\n",
    "\n",
    "      }\n",
    "\n",
    "    }\n",
    "\n",
    "  },\n",
    "\n",
    "  \"aesthetic_controls\": {\n",
    "\n",
    "    \"render_intent\": \"view expansion of source image\",\n",
    "\n",
    "    \"material_fidelity\": [\n",
    "\n",
    "    \"same as source, upscale as needed.\"\n",
    "    \n",
    "    ],\n",
    "\n",
    "    \"color_grade\": {\n",
    "\n",
    "      \"overall\": \"same as source)\",\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "  },\n",
    "\n",
    "  \"negative_prompt\": {\n",
    "\n",
    "    \"forbidden_elements\": [\n",
    "\n",
    "      \"people\",\n",
    "\n",
    "      \"animals (living)\",\n",
    "\n",
    "      \"bright neon colors\",\n",
    "\n",
    "      \"text overlays\",\n",
    "\n",
    "      \"studio backdrop\",\n",
    "\n",
    "      \"vector graphics\",\n",
    "\n",
    "      \"cartoon style\"\n",
    "\n",
    "    ]\n",
    "\n",
    "  }\n",
    "\n",
    "} \n",
    "\"\"\")\n",
    "\n",
    "#processamos as imagens baixadas, gerando novas vistas e augumentando o dataset\n",
    "process_paths(paths, input_folder, output_folder, client, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0c72b-4ff0-4008-a395-8eb82b842c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualização das imagens geradas, salva como collage.jpg no diretório atual do notebook\n",
    "create_collage(\"dataset_sinth/coxinha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b426d59-d490-46ee-ab0f-79bebc1db770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recortamos as imagens geradas em 4, salvando cada corte individualmente\n",
    "slice_images_into_four('dataset_sinth','dataset_sinth_cut')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156008f1-08ef-473a-b056-a562879dd1ad",
   "metadata": {},
   "source": [
    "### *Labeling* bruto com sam3\n",
    "\n",
    "Para treinar modelos de detecção de objetos capazes de identificar, classificar e contar elementos em imagens, é fundamental estruturar adequadamente o dataset de treinamento. \n",
    "\n",
    "Cada imagem precisa estar acompanhada de anotações que especifiquem a localização e categoria dos objetos presentes, sendo essas informações armazenadas em arquivos de labels. Existem diversos formatos consolidados na comunidade de visão computacional: \n",
    "\n",
    "- o **formato YOLO** utiliza arquivos `.txt` individuais para cada imagem, com coordenadas normalizadas no padrão `<classe> <x_centro> <y_centro> <largura> <altura>` (valores entre 0 e 1);\n",
    "- o **formato COCO** (Common Objects in Context) emprega um único arquivo JSON centralizado contendo todas as anotações do dataset, com coordenadas absolutas em pixels no formato `[x_min, y_min, largura, altura]` e suporte nativo para segmentação e keypoints;\n",
    "- o **formato Pascal VOC** (Visual Object Classes) usa arquivos XML individuais por imagem, armazenando bounding boxes como `<xmin>, <ymin>, <xmax>, <ymax>` em pixels absolutos.\n",
    "\n",
    "A escolha entre esses formatos frequentemente depende das ferramentas de anotação disponíveis e do framework de treinamento, embora conversões entre formatos sejam relativamente simples. Independentemente do formato escolhido, a qualidade e consistência das anotações são fatores determinantes para o desempenho do modelo em tarefas de detecção e contagem de objetos.\n",
    "\n",
    "Do [README do modelo](https://github.com/facebookresearch/sam3/blob/main/README.md):\n",
    "> SAM 3 is a unified foundation model for promptable segmentation in images and videos. It can detect, segment, and track objects using text or visual prompts such as points, boxes, and masks. Compared to its predecessor SAM 2, SAM 3 introduces the ability to exhaustively segment all instances of an open-vocabulary concept specified by a short text phrase or exemplars.\n",
    "\n",
    "Usamos o modelo sam3 para, com um prompt simples como 'food', 'snacks' ou 'croquette', segmentar e gerar bounding boxes para os salgadinhos presentes em uma batelada de imagens, separadas por classe. Salvamos esses resultados com .txts no formato YOLO. O conjunto de fotos de 'grupo' é salvo com a classe 'coxinha'. No proximo passo usaremos a ferramenta 'label studio' para consertar labelings equivocados.\n",
    "\n",
    "![bounding boxes geradas por sam3](relatorio2Images/sam3_raw_bbox.png)\n",
    "Acima: Bounding boxes geradas por sam3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d645dd-a8c3-4532-8dba-49c5cc5c5222",
   "metadata": {},
   "source": [
    "#### *Labeling* bruto com sam3: código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebb2d7-8ad3-4923-8ea4-5dcd0ca6bb4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### bibiliotecas e instalações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8497c8-aaee-48f0-981c-75009811149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994461f5-f7ea-40e1-8f51-608826124e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'sam3'...\n",
      "remote: Enumerating objects: 578, done.\u001b[K\n",
      "remote: Total 578 (delta 0), reused 0 (delta 0), pack-reused 578 (from 1)\u001b[K\n",
      "Receiving objects: 100% (578/578), 58.92 MiB | 10.62 MiB/s, done.\n",
      "Resolving deltas: 100% (70/70), done.\n",
      "Now in: /media/vicente/lindata/pseudoHome/00CORE/UFSC/2025Mestrado/CompVis/cv-ine410121-2025/sam3\n",
      "Now back in: /media/vicente/lindata/pseudoHome/00CORE/UFSC/2025Mestrado/CompVis/cv-ine410121-2025\n"
     ]
    }
   ],
   "source": [
    "#baixando e instalando sam3 a partir do github\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# save current dir\n",
    "start_dir = Path.cwd()\n",
    "\n",
    "# clone\n",
    "!git clone https://github.com/facebookresearch/sam3.git\n",
    "\n",
    "# go into repo\n",
    "os.chdir(\"sam3\")\n",
    "\n",
    "# install things\n",
    "!pip install -e .\n",
    "!pip install -e \".[notebooks]\"\n",
    "\n",
    "# return to original notebook dir\n",
    "os.chdir(start_dir)\n",
    "\n",
    "print(\"Now back in:\", Path.cwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9aefd-7192-4c39-b9dc-dd3decf7dd13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### imports e definições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a427480-6194-40b0-a2ab-2157ff9fa6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sam3\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "\n",
    "#utils sam3\n",
    "from huggingface_hub import login\n",
    "from sam3.sam3 import build_sam3_image_model\n",
    "from sam3.model.box_ops import box_xywh_to_cxcywh\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "from sam3.visualization_utils import normalize_bbox\n",
    "import torch\n",
    "\n",
    "#para mostrar as bounding boxes\n",
    "import glob\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d274a7f8-4293-41d0-9813-408212be889f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ae77b-3ce3-4bc1-9b1a-4f715e696a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_ext(path):\n",
    "    # Remove a extensão do arquivo do caminho fornecido\n",
    "    # Ex: \"imagem.jpg\" -> \"imagem\"\n",
    "    return os.path.splitext(path)[0]\n",
    "\n",
    "def convert_to_yolo(bbox, img_width, img_height):\n",
    "    # Desempacota as coordenadas da bounding box (canto superior esquerdo e inferior direito)\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    \n",
    "    # Calcular centro e dimensões\n",
    "    # Calcula a coordenada X do centro da bounding box\n",
    "    x_center = (x_min + x_max) / 2\n",
    "    # Calcula a coordenada Y do centro da bounding box\n",
    "    y_center = (y_min + y_max) / 2\n",
    "    # Calcula a largura da bounding box\n",
    "    width = x_max - x_min\n",
    "    # Calcula a altura da bounding box\n",
    "    height = y_max - y_min\n",
    "    \n",
    "    # Normalizar pelos tamanhos da imagem\n",
    "    # Normaliza a coordenada X do centro dividindo pela largura da imagem (valor entre 0 e 1)\n",
    "    x_center_norm = x_center / img_width\n",
    "    # Normaliza a coordenada Y do centro dividindo pela altura da imagem (valor entre 0 e 1)\n",
    "    y_center_norm = y_center / img_height\n",
    "    # Normaliza a largura da bounding box dividindo pela largura da imagem (valor entre 0 e 1)\n",
    "    width_norm = width / img_width\n",
    "    # Normaliza a altura da bounding box dividindo pela altura da imagem (valor entre 0 e 1)\n",
    "    height_norm = height / img_height\n",
    "    \n",
    "    # Retorna a bounding box no formato YOLO (centro_x, centro_y, largura, altura) normalizado\n",
    "    return [x_center_norm, y_center_norm, width_norm, height_norm]\n",
    "\n",
    "def SAM3_to_YOLO(image_path, processor, txt_prompt='snack', yolo_class=3):\n",
    "    # Remove a extensão do caminho da imagem para obter o nome base\n",
    "    image_name=strip_ext(image_path)\n",
    "    # Cria o caminho do arquivo de texto de saída com o mesmo nome da imagem\n",
    "    txt_path = f\"{image_name}.txt\" \n",
    "    \n",
    "    # Abre a imagem usando PIL\n",
    "    image = Image.open(image_path)\n",
    "    # Obtém as dimensões (largura e altura) da imagem\n",
    "    width, height = image.size\n",
    "    # Configura a imagem no processador SAM e obtém o estado de inferência\n",
    "    inference_state = processor.set_image(image)\n",
    "    # Reseta todos os prompts anteriores no estado de inferência\n",
    "    processor.reset_all_prompts(inference_state)\n",
    "    # Define o prompt de texto para detecção e obtém os resultados\n",
    "    results = processor.set_text_prompt(state=inference_state, prompt=txt_prompt)\n",
    "    # Obtém o número de objetos detectados através da quantidade de scores retornados\n",
    "    number_of_objects = len(results[\"scores\"])\n",
    "    # Inicializa string vazia para armazenar o conteúdo no formato YOLO\n",
    "    yolo_content=''\n",
    "    # Itera sobre cada objeto detectado\n",
    "    for i in range(number_of_objects):\n",
    "        # Obtém a bounding box do objeto i, converte para CPU e transforma em lista Python\n",
    "        bounding_box=results[\"boxes\"][i].cpu().tolist()\n",
    "        # Converte a bounding box para o formato YOLO normalizado\n",
    "        yolo_bbox = convert_to_yolo(bounding_box, width, height)\n",
    "        # Define o ID da classe do objeto (fixo como yolo_class)\n",
    "        class_id = yolo_class # ID da classe do objeto\n",
    "        # Formata a linha no formato YOLO: \"class_id x_center y_center width height\"\n",
    "        yolo_line = f\"{class_id} {' '.join(map(str, yolo_bbox))}\"\n",
    "    \n",
    "        # Adiciona a linha formatada ao conteúdo, seguida de quebra de linha\n",
    "        yolo_content=yolo_content+yolo_line+'\\n'\n",
    "    \n",
    "    # Abre o arquivo de texto em modo escrita\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        # Escreve todo o conteúdo YOLO no arquivo\n",
    "        f.write(yolo_content)\n",
    "    # Retorna a quantidade de objetos detectados\n",
    "    return number_of_objects\n",
    "\n",
    "def parse_yolo_txt(txt_path, img_w, img_h):\n",
    "    # Inicializa lista vazia para armazenar as bounding boxes\n",
    "    boxes = []\n",
    "    # Verifica se o arquivo de texto existe\n",
    "    if not os.path.exists(txt_path):\n",
    "        # Retorna lista vazia se o arquivo não existir\n",
    "        return boxes\n",
    "    # Abre o arquivo de texto em modo leitura\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        # Itera sobre cada linha do arquivo\n",
    "        for line in f:\n",
    "            # Remove espaços em branco no início e fim da linha\n",
    "            line = line.strip()\n",
    "            # Pula linhas vazias\n",
    "            if not line:\n",
    "                continue\n",
    "            # Divide a linha em partes separadas por espaço\n",
    "            parts = line.split()\n",
    "            # Verifica se a linha tem pelo menos 5 valores (classe + 4 coordenadas)\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            # Extrai o ID da classe e converte para inteiro\n",
    "            cls = int(float(parts[0]))\n",
    "            # Extrai a coordenada X do centro normalizada\n",
    "            x_c = float(parts[1])\n",
    "            # Extrai a coordenada Y do centro normalizada\n",
    "            y_c = float(parts[2])\n",
    "            # Extrai a largura normalizada\n",
    "            w = float(parts[3])\n",
    "            # Extrai a altura normalizada\n",
    "            h = float(parts[4])\n",
    "            # Calcula a coordenada X do canto superior esquerdo em pixels\n",
    "            x1 = int((x_c - w/2) * img_w)\n",
    "            # Calcula a coordenada Y do canto superior esquerdo em pixels\n",
    "            y1 = int((y_c - h/2) * img_h)\n",
    "            # Calcula a coordenada X do canto inferior direito em pixels\n",
    "            x2 = int((x_c + w/2) * img_w)\n",
    "            # Calcula a coordenada Y do canto inferior direito em pixels\n",
    "            y2 = int((y_c + h/2) * img_h)\n",
    "            # Limita x1 e x2 aos limites da imagem (0 até largura-1)\n",
    "            x1 = max(0, min(img_w-1, x1)); x2 = max(0, min(img_w-1, x2))\n",
    "            # Limita y1 e y2 aos limites da imagem (0 até altura-1)\n",
    "            y1 = max(0, min(img_h-1, y1)); y2 = max(0, min(img_h-1, y2))\n",
    "            # Adiciona a tupla (classe, x1, y1, x2, y2) à lista de boxes\n",
    "            boxes.append((cls, x1, y1, x2, y2))\n",
    "    # Retorna a lista de bounding boxes\n",
    "    return boxes\n",
    "\n",
    "def preview_yolo_folder(folder, max_images=20, alpha=0.5, figsize=(12,8), show_filenames=True):\n",
    "    # Converte o caminho da pasta para objeto Path\n",
    "    folder = Path(folder)\n",
    "    # Verifica se a pasta existe e se é realmente um diretório\n",
    "    if not folder.exists() or not folder.is_dir():\n",
    "        # Lança erro se a pasta não for encontrada\n",
    "        raise ValueError(f\"Folder not found: {folder}\")\n",
    "    # Define lista de extensões de imagem suportadas\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\"]\n",
    "    # Inicializa lista vazia para armazenar caminhos das imagens\n",
    "    image_paths = []\n",
    "    # Itera sobre cada extensão de arquivo\n",
    "    for e in exts:\n",
    "        # Busca todos os arquivos com a extensão atual e adiciona à lista (ordenados)\n",
    "        image_paths.extend(sorted(folder.glob(e)))\n",
    "    # Verifica se alguma imagem foi encontrada\n",
    "    if not image_paths:\n",
    "        # Lança erro se nenhuma imagem for encontrada\n",
    "        raise ValueError(f\"No images found in {folder}\")\n",
    "    # Limita o número de imagens ao máximo especificado\n",
    "    image_paths = image_paths[:max_images]\n",
    "    # Itera sobre cada caminho de imagem\n",
    "    for img_path in image_paths:\n",
    "        # Carrega a imagem usando OpenCV\n",
    "        img = cv2.imread(str(img_path))\n",
    "        # Verifica se a imagem foi carregada com sucesso\n",
    "        if img is None:\n",
    "            # Exibe mensagem de erro e pula para a próxima imagem\n",
    "            print(f\"Failed to load image: {img_path}\")\n",
    "            continue\n",
    "        # Obtém altura e largura da imagem\n",
    "        h, w = img.shape[:2]\n",
    "        # Obtém o caminho do arquivo .txt correspondente (mesmo nome, extensão .txt)\n",
    "        txt_path = img_path.with_suffix(\".txt\")\n",
    "        # Faz o parse do arquivo YOLO para obter as bounding boxes\n",
    "        boxes = parse_yolo_txt(txt_path, w, h)\n",
    "        # Cria uma cópia da imagem para o overlay (camada de cor transparente)\n",
    "        overlay = img.copy()\n",
    "        # Inicializa dicionário para armazenar cores por classe\n",
    "        colors = {}\n",
    "        # Itera sobre cada bounding box detectada\n",
    "        for box in boxes:\n",
    "            # Desempacota os valores da box (classe e coordenadas)\n",
    "            cls, x1, y1, x2, y2 = box\n",
    "            # Verifica se já existe uma cor definida para essa classe\n",
    "            if cls not in colors:\n",
    "                # Cria um gerador de números aleatórios com seed baseado na classe\n",
    "                rnd = random.Random(cls)\n",
    "                # Gera uma cor RGB aleatória (valores entre 50 e 255)\n",
    "                colors[cls] = (int(rnd.randint(50,255)), int(rnd.randint(50,255)), int(rnd.randint(50,255)))\n",
    "            # Obtém a cor para a classe atual\n",
    "            color = colors[cls]\n",
    "            # Desenha um retângulo preenchido no overlay com a cor da classe\n",
    "            cv2.rectangle(overlay, (x1,y1), (x2,y2), color, thickness=-1)\n",
    "            # Desenha a borda preta do retângulo na imagem original\n",
    "            cv2.rectangle(img, (x1,y1), (x2,y2), (0,0,0), thickness=2)\n",
    "            # Adiciona o texto com o ID da classe no canto superior esquerdo da box\n",
    "            cv2.putText(img, str(cls), (x1+3, y1+18), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2, cv2.LINE_AA)\n",
    "        # Mistura o overlay colorido com a imagem original usando o fator alpha\n",
    "        blended = cv2.addWeighted(overlay, alpha, img, 1-alpha, 0)\n",
    "        # Converte a imagem de BGR (OpenCV) para RGB (matplotlib)\n",
    "        blended_rgb = cv2.cvtColor(blended, cv2.COLOR_BGR2RGB)\n",
    "        # Verifica se deve mostrar os nomes dos arquivos\n",
    "        if show_filenames:\n",
    "            # Exibe o nome do arquivo e número de boxes em formato Markdown\n",
    "            display(Markdown(f\"**Preview:** `{img_path.name}`  —  {len(boxes)} box(es)\"))\n",
    "        # Cria uma nova figura com o tamanho especificado\n",
    "        plt.figure(figsize=figsize)\n",
    "        # Remove os eixos da visualização\n",
    "        plt.axis('off')\n",
    "        # Exibe a imagem mesclada\n",
    "        plt.imshow(blended_rgb)\n",
    "        # Renderiza a visualização\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870286d8-be4f-472e-8c00-6ba6cb9390be",
   "metadata": {},
   "source": [
    "##### execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378aeaa-b7a9-4bf6-bdb1-325ce5f71cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn on tfloat32 for Ampere GPUs\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# use bfloat16 for the entire notebook\n",
    "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed3380-e909-4cce-bbaa-b5cd2dba3728",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### construir o modelo\n",
    "[visite esta pagina para requisitar acesso pelo Hugging Face](https://huggingface.co/facebook/sam3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f7c32-78d1-48cf-b540-9cc66042225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abrir token para a API da plataforma Hugging Face\n",
    "with open('../hf_token.txt', 'r') as file:\n",
    "    my_token = file.read()\n",
    "\n",
    "#login\n",
    "login(token=my_token)\n",
    "\n",
    "#instância do modelo\n",
    "bpe_path = f\"./sam3/assets/bpe_simple_vocab_16e6.txt.gz\"\n",
    "model = build_sam3_image_model(bpe_path=bpe_path)\n",
    "processor = Sam3Processor(model, confidence_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8255ba-ff33-48c6-9f0e-1ef6147fab6b",
   "metadata": {},
   "source": [
    "###### inferencia\n",
    "esse processo foi realizado varias vezes por pasta de imagens da categoria, onde selecionamos a melhor palavra chave para o modelo encontrar o salgado nas imagens. Uma boa palavra para sanduiches não mapeia para coxinhas. \"Food\" e \"Snack\" foram os termos mais amplos identificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76d4e9-4b2f-465f-bf74-70cbd31c9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#selecionar classe para realizar o labeling em batelada\n",
    "folder = \"./dataset_sinth_cut/sanduiche\"\n",
    "\n",
    "salgados_e_classes={\"Bolinha de queijo\":0,\"Canapé\":1,\"Canudo\":2,\"Coxinha\":3,\"Croquete\":4,\"Empadinha\":5,\"Enroladinho de salsicha\":6,\"Esfiha\":7,\"Folhado\":8,\"Pastelzinho\":9,\"Pão de queijo\":10,\"Quibe\":11,\"Risoles\":12,\"Sanduiche\":13}\n",
    "\n",
    "#selecionar o salgado da pasta, o dicionário vai mapear para o encoding YOLO\n",
    "set_class = salgados_e_classes[\"Sanduiche\"]\n",
    "\n",
    "# valid image extensions\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".webp\"}\n",
    "\n",
    "#habilita uma segunda tentativa com um termo mais amplo caso o modelo não crie nenhum label com o prompt principal\n",
    "second_try=True\n",
    "\n",
    "#file iterator\n",
    "for root, _, files in os.walk(folder):\n",
    "    for f in files:\n",
    "        if os.path.splitext(f)[1].lower() in exts:\n",
    "            path = os.path.join(root, f)\n",
    "            total = SAM3_to_YOLO(path, \n",
    "                                 processor, \n",
    "                                 txt_prompt='sandwich', #main prompt\n",
    "                                 yolo_class = set_class)\n",
    "            if total == 0 and second_try==True:\n",
    "                print('0 ->', end = ' ')\n",
    "                total = SAM3_to_YOLO(path, \n",
    "                                 processor, \n",
    "                                 txt_prompt='hors d’oeuvres', #fallback prompt\n",
    "                                 yolo_class = set_class)\n",
    "            print(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3918bd9-e306-4bb7-8f09-a10bdf0291d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the labels\n",
    "folder = \"./dataset_sinth_cut/sanduiche\"\n",
    "preview_yolo_folder(folder, max_images=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a23d3-8547-4023-8abf-4b2663d65841",
   "metadata": {},
   "source": [
    "### Refinamento de labels com Label Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89eab6f-77d0-40b8-8faf-b1dc6d960d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
