{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ced7d0-bf03-43ba-932a-3cfed166cfff",
   "metadata": {},
   "source": [
    "# Classificação e contagem de Salgadinhos com Deep Learning\n",
    "Autores: Vicente Knobel Borges e Luiz Gustavo Xavier\n",
    "\n",
    "Neste relatório é apresentada a solução de um problema de visão computacional aplicado a indústria de alimentos, como requisitos para a disciplina INE410121 - Visão Computacional. O projeto consiste na aplicação de algoritmos clássicos de visão computacional e redes neurais artificiais para solução do problema de contagem de salgadinhos sortidos em caixas mistas. Em nossa melhor solução com o *fine tune* do modelo pré-treinado [RF-DETR](https://github.com/roboflow/rf-detr), foi possível identificar corretamente diversas classes de salgadinhos em fotografias diversas de caixas, obtendo um mAP@50:95 de 90,51% no teste com nosso dataset.\n",
    "\n",
    "O processo de geração do modelo se deu por uma sequencia de passos:\n",
    "1. Busca manual de imagens exemplo no google imagens\n",
    "2. Geração de [*novel view synthesis*](https://en.wikipedia.org/wiki/View_synthesis) das imagens adquiridas usando o modelo [Nano Banana Pro](https://blog.google/technology/ai/nano-banana-pro/) da Google.\n",
    "3. *Labeling* bruto usando [sam3](https://github.com/facebookresearch/sam3)\n",
    "4. Refinamento das labels usando [Label Studio](https://labelstud.io/)\n",
    "5. Treinamento usando modelos [RF-DETR](https://github.com/roboflow/rf-detr) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c032cd-cd3f-4355-83db-19795597636a",
   "metadata": {},
   "source": [
    "## Geração do *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3d39a-27c1-49bf-92f9-36d9c1846983",
   "metadata": {},
   "source": [
    "### Busca manual pelo google imagens\n",
    "Começamos o processo definindo quais salgadinhos o modelo deve indentificar. Buscamos representar os salgadinhos mais comuns vendidos por empresas de alimentos para festas, chegando na lista:\n",
    "- Bolinha de queijo\n",
    "- Canapé\n",
    "- Canudo\n",
    "- Coxinha\n",
    "- Croquete\n",
    "- Empadinha\n",
    "- Enroladinho de salsicha\n",
    "- Esfiha\n",
    "- Folhado\n",
    "- Pastelzinho\n",
    "- Pão de queijo\n",
    "- Quibe\n",
    "- Risoles\n",
    "- Sanduiche\n",
    "\n",
    "Buscamos 30 imagens de cada classe, além de um conjunto de imagens de bandejas cheias, denominado 'grupo'. A Heuristica de inclusão priorizava a escolha de imagens por:\n",
    "- Conter multiplos exemplares do salgadinho buscado em uma unica foto\n",
    "- Salgadinhos fotografados no angulo *top-down*\n",
    "- Salgadinhos em bandejas são preferenciais a salgadinhos em potes ou empilhados\n",
    "\n",
    "![imagens de coxinhas selecionadas](relatorio2Images/coxinhas_google.jpg)\n",
    "Acima: Imagens de coxinhas selecionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c30516-ede7-410c-aee9-ec75a815fa25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Novel view synthesis com Nano Banana Pro\n",
    "No processo de buscar exemplos no google, nos deparamos com um forte viés em fotos de uma perspectiva lateral ao corpo de salgados, coerente com propagandas e divulgação. Embora seja relevante que nosso modelo trabalhe com este outro angulo de visão, achamos preocupante a ausência de fotos de vista superior.\n",
    "\n",
    "A vista superior é a perspectiva que estipulamos como nosso caso de uso principal com o modelo sendo colocado em produção por padeiros interessados em contar os salgados em suas caixas antes de despachar para entrega.\n",
    "\n",
    "Para gerar essas vistas fizemos uso do novo modelo de geração, edição e manipulação de imagens da Google, o [Nano Banana Pro](https://blog.google/technology/ai/nano-banana-pro/).\n",
    "\n",
    "O modelo está disponivel pelo [serviço de API dos modelos Gemini](https://ai.google.dev/gemini-api/docs/image-generation) e por meio de *prompt engineering*, implementamos uma geração que extrapola quatro novas imagens a partir da imagem original, em uma unica geração.\n",
    "\n",
    "- Uma geração de vista lateral, similar a imagem original;\n",
    "- Uma geração top-down, dos salgadinhos no chão - mudando o fundo dos salgadinhos e variando o fundo;\n",
    "- Uma geração top-down, com luz noturna, diversificando condições de iluminação\n",
    "- Uma geração top-down dos salgados na sua disposição original.\n",
    "\n",
    "![imagens de coxinhas geradas pelo modelo Nano Banana](relatorio2Images/nanoBanana.jpg)\n",
    "Acima: Imagens de coxinhas geradas pelo modelo Nano Banana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ffafa3-2585-47ce-b77a-07dde7217770",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Novel View Synthesis: código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2053493f-57da-492f-abb8-c86bb59eeaa3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c09e891-3d86-4e69-b2cb-780e88508950",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U google-genai #api para acesso aos modelos de IA da Google\n",
    "!pip install -U pillow #bibilioteca de manipulação de imagens\n",
    "!pip install -U opencv-python #bibilioteca de visão computacional\n",
    "!pip install -U tqdm #progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84589b86-3e27-4827-b925-fbab9a25da24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### imports e definições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6608da14-60fb-44f5-8ae6-3a40a5610322",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key='' #your google api key here\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e48464-8d42-457f-a68c-82f3d5bebdae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d79e98b-0837-4cac-adee-773198b7c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nanoBananaGeneration(client, prompt, image_path_in, image_path_out):\n",
    "    # Define a proporção de aspecto da imagem gerada\n",
    "    aspect_ratio = \"5:4\" # \"1:1\",\"2:3\",\"3:2\",\"3:4\",\"4:3\",\"4:5\",\"5:4\",\"9:16\",\"16:9\",\"21:9\"\n",
    "    # Define a resolução da imagem gerada\n",
    "    resolution = \"1K\" # \"1K\", \"2K\", \"4K\"\n",
    "    \n",
    "    # Faz uma chamada ao modelo Gemini para gerar conteúdo (texto e imagem)\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-3-pro-image-preview\",\n",
    "        # Passa o prompt de texto e a imagem de entrada como conteúdo\n",
    "        contents=[\n",
    "            prompt,\n",
    "            Image.open(image_path_in),\n",
    "        ],\n",
    "        # Configura os parâmetros de geração\n",
    "        config=types.GenerateContentConfig(\n",
    "            # Define que a resposta pode conter texto e imagem\n",
    "            response_modalities=['TEXT', 'IMAGE'],\n",
    "            # Configura as propriedades da imagem a ser gerada\n",
    "            image_config=types.ImageConfig(\n",
    "                aspect_ratio=aspect_ratio,\n",
    "                image_size=resolution\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Itera sobre as partes da resposta gerada\n",
    "    for part in response.parts:\n",
    "        # Se a parte contém texto, imprime no console\n",
    "        if part.text is not None:\n",
    "            print(part.text)\n",
    "        # Se a parte contém uma imagem, salva no caminho de saída\n",
    "        elif image:= part.as_image():\n",
    "            image.save(image_path_out)\n",
    "\n",
    "def collect_image_paths(input_folder):\n",
    "    # Define as extensões de arquivo de imagem aceitas\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "    # Inicializa lista vazia para armazenar os caminhos\n",
    "    paths = []\n",
    "    # Percorre recursivamente todos os arquivos e subpastas da pasta de entrada\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        # Para cada arquivo encontrado\n",
    "        for f in files:\n",
    "            # Verifica se a extensão do arquivo está na lista de extensões aceitas (ignorando maiúsculas/minúsculas)\n",
    "            if Path(f).suffix.lower() in exts:\n",
    "                # Adiciona o caminho completo do arquivo à lista\n",
    "                paths.append(Path(root) / f)\n",
    "    # Retorna a lista de caminhos das imagens encontradas\n",
    "    return paths\n",
    "\n",
    "def filter_unprocessed(paths, input_folder, output_folder):\n",
    "    # Converte as pastas de entrada e saída para objetos Path\n",
    "    input_folder = Path(input_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    # Inicializa lista para armazenar apenas os caminhos não processados\n",
    "    filtered = []\n",
    "    # Para cada caminho de imagem de origem\n",
    "    for src_path in paths:\n",
    "        # Obtém o caminho relativo da imagem em relação à pasta de entrada\n",
    "        rel = src_path.relative_to(input_folder)\n",
    "        # Remove a extensão do caminho relativo\n",
    "        rel_no_ext = rel.with_suffix(\"\")\n",
    "        # Constrói o caminho de destino na pasta de saída com extensão .jpg\n",
    "        dst_path = (output_folder / rel_no_ext).with_suffix(\".jpg\")\n",
    "        # Se o arquivo de destino não existe, adiciona à lista de não processados\n",
    "        if not dst_path.exists():\n",
    "            filtered.append(src_path)\n",
    "    # Retorna apenas os caminhos que ainda não foram processados\n",
    "    return filtered\n",
    "\n",
    "def process_paths(paths, input_folder, output_folder, client, prompt):\n",
    "    # Converte as pastas de entrada e saída para objetos Path\n",
    "    input_folder = Path(input_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    \n",
    "    # Garante que a pasta de saída raiz existe (cria se não existir)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    # Itera sobre os caminhos com barra de progresso usando tqdm\n",
    "    for src_path in tqdm(paths, desc=\"Processing images\"):\n",
    "        # Obtém o caminho relativo da imagem em relação à pasta de entrada\n",
    "        rel_path = src_path.relative_to(input_folder)\n",
    "        # Remove a extensão do caminho relativo\n",
    "        rel_no_ext = rel_path.with_suffix(\"\")\n",
    "        # Constrói o caminho de destino na pasta de saída com extensão .jpg\n",
    "        dst_path = (output_folder / rel_no_ext).with_suffix(\".jpg\")\n",
    "        # Garante que as subpastas necessárias existem no destino\n",
    "        dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Tenta processar a imagem\n",
    "        try:\n",
    "            # Abre a imagem de origem e converte para RGB\n",
    "            img = Image.open(src_path).convert(\"RGB\")\n",
    "            # Chama a função de geração com o cliente, prompt e caminhos\n",
    "            nanoBananaGeneration(client, prompt, src_path, dst_path)\n",
    "        # Se houver qualquer erro, ignora e continua para a próxima imagem\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "def resize_to_box(img, box_w=1440, box_h=720):\n",
    "    # Obtém a altura e largura da imagem\n",
    "    h, w = img.shape[:2]\n",
    "    # Calcula o fator de escala para caber na caixa, mantendo proporção (usa o menor dos dois)\n",
    "    scale = min(box_w / w, box_h / h)\n",
    "    # Calcula a nova largura aplicando o fator de escala\n",
    "    new_w = int(w * scale)\n",
    "    # Calcula a nova altura aplicando o fator de escala\n",
    "    new_h = int(h * scale)\n",
    "    # Redimensiona a imagem com interpolação INTER_AREA (boa para redução)\n",
    "    return cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def create_collage(input_folder, out_path=\"collage.jpg\", box_w=1440, box_h=720):\n",
    "    # Converte a pasta de entrada para objeto Path\n",
    "    input_folder = Path(input_folder)\n",
    "    # Obtém lista ordenada de todos os caminhos de arquivos na pasta\n",
    "    image_paths = sorted(glob.glob(str(input_folder / \"*\")))\n",
    "    # Inicializa lista vazia para armazenar as imagens carregadas\n",
    "    images = []\n",
    "    # Para cada caminho de imagem\n",
    "    for p in image_paths:\n",
    "        # Tenta carregar a imagem usando OpenCV\n",
    "        img = cv2.imread(p)\n",
    "        # Se a imagem não pôde ser carregada, pula para a próxima\n",
    "        if img is None:\n",
    "            continue\n",
    "        # Redimensiona a imagem para caber na caixa definida\n",
    "        img = resize_to_box(img, box_w, box_h)\n",
    "        # Adiciona a imagem redimensionada à lista\n",
    "        images.append(img)\n",
    "    # Se nenhuma imagem foi carregada, imprime mensagem e sai\n",
    "    if not images:\n",
    "        print(\"No images found.\")\n",
    "        return\n",
    "    # --- constrói grade automaticamente ---\n",
    "    # Conta o número total de imagens\n",
    "    n = len(images)\n",
    "    # Calcula o número de colunas (arredonda para cima a raiz quadrada)\n",
    "    cols = math.ceil(math.sqrt(n))\n",
    "    # Calcula o número de linhas necessárias\n",
    "    rows = math.ceil(n / cols)\n",
    "    # Define largura e altura de cada célula da grade\n",
    "    cell_w, cell_h = box_w, box_h\n",
    "    # Calcula largura total da colagem\n",
    "    collage_w = cols * cell_w\n",
    "    # Calcula altura total da colagem\n",
    "    collage_h = rows * cell_h\n",
    "    # Cria um array numpy vazio (preto) para a colagem completa\n",
    "    collage = np.zeros((collage_h, collage_w, 3), dtype=np.uint8)\n",
    "    # Inicializa índice da imagem atual\n",
    "    i = 0\n",
    "    # Para cada linha da grade\n",
    "    for r in range(rows):\n",
    "        # Para cada coluna da grade\n",
    "        for c in range(cols):\n",
    "            # Se já processou todas as imagens, interrompe\n",
    "            if i >= n: break\n",
    "            # Pega a imagem atual\n",
    "            img = images[i]\n",
    "            # Calcula o deslocamento vertical para centralizar a imagem na célula\n",
    "            y_offset = r * cell_h + (cell_h - img.shape[0]) // 2\n",
    "            # Calcula o deslocamento horizontal para centralizar a imagem na célula\n",
    "            x_offset = c * cell_w + (cell_w - img.shape[1]) // 2\n",
    "            # Coloca a imagem na posição calculada dentro da colagem\n",
    "            collage[y_offset:y_offset+img.shape[0],\n",
    "                    x_offset:x_offset+img.shape[1]] = img\n",
    "            # Avança para a próxima imagem\n",
    "            i += 1\n",
    "\n",
    "    # Redimensiona a colagem para ter o tamanho final da caixa de interesse\n",
    "    collage = resize_to_box(collage, box_w, box_h)\n",
    "    # Salva a colagem final no caminho especificado\n",
    "    cv2.imwrite(out_path, collage)\n",
    "    # Imprime mensagem confirmando o salvamento\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "def slice_images_into_four(input_folder, output_folder=\"imagens_cortadas\"):\n",
    "    # Converte as pastas de entrada e saída para objetos Path\n",
    "    input_folder = Path(input_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    \n",
    "    # Define as extensões de arquivo de imagem aceitas\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "    \n",
    "    # Garante que a pasta de saída raiz existe (cria se não existir)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Coleta todos os caminhos de imagens recursivamente\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        for f in files:\n",
    "            if Path(f).suffix.lower() in exts:\n",
    "                image_paths.append(Path(root) / f)\n",
    "    \n",
    "    # Itera sobre os caminhos com barra de progresso usando tqdm\n",
    "    for src_path in tqdm(image_paths, desc=\"Slicing images\"):\n",
    "        # Tenta processar a imagem\n",
    "        try:\n",
    "            # Carrega a imagem usando OpenCV\n",
    "            img = cv2.imread(str(src_path))\n",
    "            # Se a imagem não pôde ser carregada, pula para a próxima\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Obtém a altura e largura da imagem\n",
    "            h, w = img.shape[:2]\n",
    "            # Calcula a altura de cada fatia (metade da altura total)\n",
    "            half_h = h // 2\n",
    "            # Calcula a largura de cada fatia (metade da largura total)\n",
    "            half_w = w // 2\n",
    "            \n",
    "            # Define as 4 fatias da imagem (superior esquerda, superior direita, inferior esquerda, inferior direita)\n",
    "            slices = [\n",
    "                img[0:half_h, 0:half_w],           # fatia 0: superior esquerda\n",
    "                img[0:half_h, half_w:w],           # fatia 1: superior direita\n",
    "                img[half_h:h, 0:half_w],           # fatia 2: inferior esquerda\n",
    "                img[half_h:h, half_w:w]            # fatia 3: inferior direita\n",
    "            ]\n",
    "            \n",
    "            # Obtém o caminho relativo da imagem em relação à pasta de entrada\n",
    "            rel_path = src_path.relative_to(input_folder)\n",
    "            # Obtém o caminho relativo sem a extensão\n",
    "            rel_no_ext = rel_path.with_suffix(\"\")\n",
    "            \n",
    "            # Para cada fatia (0 a 3)\n",
    "            for idx, slice_img in enumerate(slices):\n",
    "                # Constrói o caminho de destino adicionando o índice da fatia ao nome\n",
    "                dst_path = (output_folder / f\"{rel_no_ext}_{idx}\").with_suffix(\".jpg\")\n",
    "                # Garante que as subpastas necessárias existem no destino\n",
    "                dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                # Salva a fatia da imagem\n",
    "                cv2.imwrite(str(dst_path), slice_img)\n",
    "        \n",
    "        # Se houver qualquer erro, ignora e continua para a próxima imagem\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {src_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Imprime mensagem de conclusão\n",
    "    print(f\"Slicing complete! Images saved to: {output_folder}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0526bb-0b8d-4d66-b918-1e6a7e3cf863",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### execução\n",
    "Folder com as imagens baixadas deve estar na estrutura\n",
    "```\n",
    "dataset/\n",
    "├── coxinha/\n",
    "│   ├── coxinha0001.jpg\n",
    "│   ├── coxinha0002.jpg\n",
    "│   ├── coxinha0003.jpg\n",
    "│   └── ...\n",
    "├── bolinha_de_queijo/\n",
    "│   ├── bolinha0001.jpg\n",
    "│   ├── bolinha0002.jpg\n",
    "│   └── ...\n",
    "└── pastel/\n",
    "    ├── pastel0001.jpg\n",
    "    ├── pastel0002.jpg\n",
    "    └── ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65222b96-9103-46c5-81e5-a39fcd637443",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"dataset\"\n",
    "output_folder = \"dataset_sinth\"\n",
    "\n",
    "#coleta paths de todas as imagens\n",
    "paths = collect_image_paths(input_folder) \n",
    "\n",
    "#remove da lista imagens do qual as vistas sintéticas já foram criadas\n",
    "paths = filter_unprocessed(paths, input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd487e52-b3d8-4f51-a0e4-f03dfa0754e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprime os paths das imagens baixadas do google\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398dab4b-aebd-42f3-9eb9-e0cb336ddbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o cliente recebe a chave da API declarada acima\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "#o prompt detalha as visadas da imagem apresentada que nos interessam.\n",
    "prompt = (\"\"\"\n",
    "Hi! I'm doing a image training dataset of Brazilian Salgadinhos, and want to generate novel views of the image i sent you. Follow the generation guide below please!\n",
    " {\n",
    "\n",
    "  \"style_mode\": \"raw_photoreal_documentary_collage\",\n",
    "\n",
    "  \"look\": \"casual product photography, domestic setting, natural and ambient lighting, 2x2 grid layout\",\n",
    "\n",
    "  \"layout_structure\": {\n",
    "\n",
    "    \"format\": \"four-panel collage (2x2 grid)\",\n",
    "\n",
    "    \"description\": \"The image is divided into four distinct rectangular quadrants, each presenting a different angle and lighting condition of the subject on the source image.\"\n",
    "\n",
    "  },\n",
    "\n",
    "  \"camera\": {\n",
    "\n",
    "    \"vantage\": \"variable per quadrant (high-angle, eye-level, low-angle, top-down)\",\n",
    "\n",
    "    \"framing\": \"medium to close-up shots\",\n",
    "\n",
    "    \"lens_behavior\": \"smartphone camera aesthetic, varying depth of field (shallow in macro shots, deep in environmental shots)\",\n",
    "\n",
    "    \"sensor_quality\": \"standard digital photography, slight ISO noise in darker areas, realistic sharpness\"\n",
    "\n",
    "  },\n",
    "\n",
    "  \"scene\": {\n",
    "\n",
    "    \"quadrant_details\": {\n",
    "\n",
    "      \"top_left\": {\n",
    "\n",
    "        \"perspective\": \"high-angle three-quarter view\",\n",
    "\n",
    "        \"lighting\": \"same as the source image\",\n",
    "\n",
    "        \"background\": \"same as the source image\",\n",
    "\n",
    "      },\n",
    "\n",
    "      \"top_right\": {\n",
    "\n",
    "        \"perspective\": \"direct top-down (flat lay)\",\n",
    "\n",
    "        \"lighting\": \"same as the source image\",\n",
    "\n",
    "        \"background\": \"Salgadinhos on the image out of the tray and on the floor\",\n",
    "\n",
    "\n",
    "      },\n",
    "\n",
    "      \"bottom_left\": {\n",
    "\n",
    "        \"perspective\": \"direct top-down (flat lay)\",\n",
    "\n",
    "        \"lighting\": \"nightime lighting\",\n",
    "\n",
    "        \"background\": \"same as the source image\",\n",
    "\n",
    "      },\n",
    "\n",
    "      \"bottom_right\": {\n",
    "\n",
    "        \"perspective\": \"direct top-down (flat lay)\",\n",
    "\n",
    "        \"lighting\": \"same as the source image\",\n",
    "\n",
    "        \"background\": \"same as the source image\",\n",
    "\n",
    "\n",
    "      }\n",
    "\n",
    "    }\n",
    "\n",
    "  },\n",
    "\n",
    "  \"aesthetic_controls\": {\n",
    "\n",
    "    \"render_intent\": \"view expansion of source image\",\n",
    "\n",
    "    \"material_fidelity\": [\n",
    "\n",
    "    \"same as source, upscale as needed.\"\n",
    "    \n",
    "    ],\n",
    "\n",
    "    \"color_grade\": {\n",
    "\n",
    "      \"overall\": \"same as source)\",\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "  },\n",
    "\n",
    "  \"negative_prompt\": {\n",
    "\n",
    "    \"forbidden_elements\": [\n",
    "\n",
    "      \"people\",\n",
    "\n",
    "      \"animals (living)\",\n",
    "\n",
    "      \"bright neon colors\",\n",
    "\n",
    "      \"text overlays\",\n",
    "\n",
    "      \"studio backdrop\",\n",
    "\n",
    "      \"vector graphics\",\n",
    "\n",
    "      \"cartoon style\"\n",
    "\n",
    "    ]\n",
    "\n",
    "  }\n",
    "\n",
    "} \n",
    "\"\"\")\n",
    "\n",
    "#processamos as imagens baixadas, gerando novas vistas e augumentando o dataset\n",
    "process_paths(paths, input_folder, output_folder, client, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0c72b-4ff0-4008-a395-8eb82b842c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualização das imagens geradas, salva como collage.jpg no diretório atual do notebook\n",
    "create_collage(\"dataset_sinth/coxinha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b426d59-d490-46ee-ab0f-79bebc1db770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recortamos as imagens geradas em 4, salvando cada corte individualmente\n",
    "slice_images_into_four('dataset_sinth','dataset_sinth_cut')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156008f1-08ef-473a-b056-a562879dd1ad",
   "metadata": {},
   "source": [
    "### *Labeling* bruto com sam3\n",
    "\n",
    "Para treinar modelos de detecção de objetos capazes de identificar, classificar e contar elementos em imagens, é fundamental estruturar adequadamente o dataset de treinamento. \n",
    "\n",
    "Cada imagem precisa estar acompanhada de anotações que especifiquem a localização e categoria dos objetos presentes, sendo essas informações armazenadas em arquivos de labels. Existem diversos formatos consolidados na comunidade de visão computacional: \n",
    "\n",
    "- o **formato YOLO** utiliza arquivos `.txt` individuais para cada imagem, com coordenadas normalizadas no padrão `<classe> <x_centro> <y_centro> <largura> <altura>` (valores entre 0 e 1);\n",
    "- o **formato COCO** (Common Objects in Context) emprega um único arquivo JSON centralizado contendo todas as anotações do dataset, com coordenadas absolutas em pixels no formato `[x_min, y_min, largura, altura]` e suporte nativo para segmentação e keypoints;\n",
    "- o **formato Pascal VOC** (Visual Object Classes) usa arquivos XML individuais por imagem, armazenando bounding boxes como `<xmin>, <ymin>, <xmax>, <ymax>` em pixels absolutos.\n",
    "\n",
    "A escolha entre esses formatos frequentemente depende das ferramentas de anotação disponíveis e do framework de treinamento, embora conversões entre formatos sejam relativamente simples. Independentemente do formato escolhido, a qualidade e consistência das anotações são fatores determinantes para o desempenho do modelo em tarefas de detecção e contagem de objetos.\n",
    "\n",
    "Do [README do modelo](https://github.com/facebookresearch/sam3/blob/main/README.md):\n",
    "> SAM 3 is a unified foundation model for promptable segmentation in images and videos. It can detect, segment, and track objects using text or visual prompts such as points, boxes, and masks. Compared to its predecessor SAM 2, SAM 3 introduces the ability to exhaustively segment all instances of an open-vocabulary concept specified by a short text phrase or exemplars.\n",
    "\n",
    "Usamos o modelo sam3 para, com um prompt simples como 'food', 'snacks' ou 'croquette', segmentar e gerar bounding boxes para os salgadinhos presentes em uma batelada de imagens, separadas por classe. Salvamos esses resultados com .txts no formato YOLO. O conjunto de fotos de 'grupo' é salvo com a classe 'coxinha'. No proximo passo usaremos a ferramenta 'label studio' para consertar labelings equivocados.\n",
    "\n",
    "![bounding boxes geradas por sam3](relatorio2Images/sam3_raw_bbox.png)\n",
    "Acima: Bounding boxes geradas por sam3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d645dd-a8c3-4532-8dba-49c5cc5c5222",
   "metadata": {},
   "source": [
    "#### *Labeling* bruto com sam3: código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebb2d7-8ad3-4923-8ea4-5dcd0ca6bb4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### bibiliotecas e instalações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8497c8-aaee-48f0-981c-75009811149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994461f5-f7ea-40e1-8f51-608826124e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'sam3'...\n",
      "remote: Enumerating objects: 578, done.\u001b[K\n",
      "remote: Total 578 (delta 0), reused 0 (delta 0), pack-reused 578 (from 1)\u001b[K\n",
      "Receiving objects: 100% (578/578), 58.92 MiB | 10.62 MiB/s, done.\n",
      "Resolving deltas: 100% (70/70), done.\n",
      "Now in: /media/vicente/lindata/pseudoHome/00CORE/UFSC/2025Mestrado/CompVis/cv-ine410121-2025/sam3\n",
      "Now back in: /media/vicente/lindata/pseudoHome/00CORE/UFSC/2025Mestrado/CompVis/cv-ine410121-2025\n"
     ]
    }
   ],
   "source": [
    "#baixando e instalando sam3 a partir do github\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# save current dir\n",
    "start_dir = Path.cwd()\n",
    "\n",
    "# clone\n",
    "!git clone https://github.com/facebookresearch/sam3.git\n",
    "\n",
    "# go into repo\n",
    "os.chdir(\"sam3\")\n",
    "\n",
    "# install things\n",
    "!pip install -e .\n",
    "!pip install -e \".[notebooks]\"\n",
    "\n",
    "# return to original notebook dir\n",
    "os.chdir(start_dir)\n",
    "\n",
    "print(\"Now back in:\", Path.cwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9aefd-7192-4c39-b9dc-dd3decf7dd13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### imports e definições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a427480-6194-40b0-a2ab-2157ff9fa6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sam3\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "\n",
    "#utils sam3\n",
    "from huggingface_hub import login\n",
    "from sam3.sam3 import build_sam3_image_model\n",
    "from sam3.model.box_ops import box_xywh_to_cxcywh\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "from sam3.visualization_utils import normalize_bbox\n",
    "import torch\n",
    "\n",
    "#para mostrar as bounding boxes\n",
    "import glob\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d274a7f8-4293-41d0-9813-408212be889f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ae77b-3ce3-4bc1-9b1a-4f715e696a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_ext(path):\n",
    "    # Remove a extensão do arquivo do caminho fornecido\n",
    "    # Ex: \"imagem.jpg\" -> \"imagem\"\n",
    "    return os.path.splitext(path)[0]\n",
    "\n",
    "def convert_to_yolo(bbox, img_width, img_height):\n",
    "    # Desempacota as coordenadas da bounding box (canto superior esquerdo e inferior direito)\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    \n",
    "    # Calcular centro e dimensões\n",
    "    # Calcula a coordenada X do centro da bounding box\n",
    "    x_center = (x_min + x_max) / 2\n",
    "    # Calcula a coordenada Y do centro da bounding box\n",
    "    y_center = (y_min + y_max) / 2\n",
    "    # Calcula a largura da bounding box\n",
    "    width = x_max - x_min\n",
    "    # Calcula a altura da bounding box\n",
    "    height = y_max - y_min\n",
    "    \n",
    "    # Normalizar pelos tamanhos da imagem\n",
    "    # Normaliza a coordenada X do centro dividindo pela largura da imagem (valor entre 0 e 1)\n",
    "    x_center_norm = x_center / img_width\n",
    "    # Normaliza a coordenada Y do centro dividindo pela altura da imagem (valor entre 0 e 1)\n",
    "    y_center_norm = y_center / img_height\n",
    "    # Normaliza a largura da bounding box dividindo pela largura da imagem (valor entre 0 e 1)\n",
    "    width_norm = width / img_width\n",
    "    # Normaliza a altura da bounding box dividindo pela altura da imagem (valor entre 0 e 1)\n",
    "    height_norm = height / img_height\n",
    "    \n",
    "    # Retorna a bounding box no formato YOLO (centro_x, centro_y, largura, altura) normalizado\n",
    "    return [x_center_norm, y_center_norm, width_norm, height_norm]\n",
    "\n",
    "def SAM3_to_YOLO(image_path, processor, txt_prompt='snack', yolo_class=3):\n",
    "    # Remove a extensão do caminho da imagem para obter o nome base\n",
    "    image_name=strip_ext(image_path)\n",
    "    # Cria o caminho do arquivo de texto de saída com o mesmo nome da imagem\n",
    "    txt_path = f\"{image_name}.txt\" \n",
    "    \n",
    "    # Abre a imagem usando PIL\n",
    "    image = Image.open(image_path)\n",
    "    # Obtém as dimensões (largura e altura) da imagem\n",
    "    width, height = image.size\n",
    "    # Configura a imagem no processador SAM e obtém o estado de inferência\n",
    "    inference_state = processor.set_image(image)\n",
    "    # Reseta todos os prompts anteriores no estado de inferência\n",
    "    processor.reset_all_prompts(inference_state)\n",
    "    # Define o prompt de texto para detecção e obtém os resultados\n",
    "    results = processor.set_text_prompt(state=inference_state, prompt=txt_prompt)\n",
    "    # Obtém o número de objetos detectados através da quantidade de scores retornados\n",
    "    number_of_objects = len(results[\"scores\"])\n",
    "    # Inicializa string vazia para armazenar o conteúdo no formato YOLO\n",
    "    yolo_content=''\n",
    "    # Itera sobre cada objeto detectado\n",
    "    for i in range(number_of_objects):\n",
    "        # Obtém a bounding box do objeto i, converte para CPU e transforma em lista Python\n",
    "        bounding_box=results[\"boxes\"][i].cpu().tolist()\n",
    "        # Converte a bounding box para o formato YOLO normalizado\n",
    "        yolo_bbox = convert_to_yolo(bounding_box, width, height)\n",
    "        # Define o ID da classe do objeto (fixo como yolo_class)\n",
    "        class_id = yolo_class # ID da classe do objeto\n",
    "        # Formata a linha no formato YOLO: \"class_id x_center y_center width height\"\n",
    "        yolo_line = f\"{class_id} {' '.join(map(str, yolo_bbox))}\"\n",
    "    \n",
    "        # Adiciona a linha formatada ao conteúdo, seguida de quebra de linha\n",
    "        yolo_content=yolo_content+yolo_line+'\\n'\n",
    "    \n",
    "    # Abre o arquivo de texto em modo escrita\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        # Escreve todo o conteúdo YOLO no arquivo\n",
    "        f.write(yolo_content)\n",
    "    # Retorna a quantidade de objetos detectados\n",
    "    return number_of_objects\n",
    "\n",
    "def parse_yolo_txt(txt_path, img_w, img_h):\n",
    "    # Inicializa lista vazia para armazenar as bounding boxes\n",
    "    boxes = []\n",
    "    # Verifica se o arquivo de texto existe\n",
    "    if not os.path.exists(txt_path):\n",
    "        # Retorna lista vazia se o arquivo não existir\n",
    "        return boxes\n",
    "    # Abre o arquivo de texto em modo leitura\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        # Itera sobre cada linha do arquivo\n",
    "        for line in f:\n",
    "            # Remove espaços em branco no início e fim da linha\n",
    "            line = line.strip()\n",
    "            # Pula linhas vazias\n",
    "            if not line:\n",
    "                continue\n",
    "            # Divide a linha em partes separadas por espaço\n",
    "            parts = line.split()\n",
    "            # Verifica se a linha tem pelo menos 5 valores (classe + 4 coordenadas)\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            # Extrai o ID da classe e converte para inteiro\n",
    "            cls = int(float(parts[0]))\n",
    "            # Extrai a coordenada X do centro normalizada\n",
    "            x_c = float(parts[1])\n",
    "            # Extrai a coordenada Y do centro normalizada\n",
    "            y_c = float(parts[2])\n",
    "            # Extrai a largura normalizada\n",
    "            w = float(parts[3])\n",
    "            # Extrai a altura normalizada\n",
    "            h = float(parts[4])\n",
    "            # Calcula a coordenada X do canto superior esquerdo em pixels\n",
    "            x1 = int((x_c - w/2) * img_w)\n",
    "            # Calcula a coordenada Y do canto superior esquerdo em pixels\n",
    "            y1 = int((y_c - h/2) * img_h)\n",
    "            # Calcula a coordenada X do canto inferior direito em pixels\n",
    "            x2 = int((x_c + w/2) * img_w)\n",
    "            # Calcula a coordenada Y do canto inferior direito em pixels\n",
    "            y2 = int((y_c + h/2) * img_h)\n",
    "            # Limita x1 e x2 aos limites da imagem (0 até largura-1)\n",
    "            x1 = max(0, min(img_w-1, x1)); x2 = max(0, min(img_w-1, x2))\n",
    "            # Limita y1 e y2 aos limites da imagem (0 até altura-1)\n",
    "            y1 = max(0, min(img_h-1, y1)); y2 = max(0, min(img_h-1, y2))\n",
    "            # Adiciona a tupla (classe, x1, y1, x2, y2) à lista de boxes\n",
    "            boxes.append((cls, x1, y1, x2, y2))\n",
    "    # Retorna a lista de bounding boxes\n",
    "    return boxes\n",
    "\n",
    "def preview_yolo_folder(folder, max_images=20, alpha=0.5, figsize=(12,8), show_filenames=True):\n",
    "    # Converte o caminho da pasta para objeto Path\n",
    "    folder = Path(folder)\n",
    "    # Verifica se a pasta existe e se é realmente um diretório\n",
    "    if not folder.exists() or not folder.is_dir():\n",
    "        # Lança erro se a pasta não for encontrada\n",
    "        raise ValueError(f\"Folder not found: {folder}\")\n",
    "    # Define lista de extensões de imagem suportadas\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\"]\n",
    "    # Inicializa lista vazia para armazenar caminhos das imagens\n",
    "    image_paths = []\n",
    "    # Itera sobre cada extensão de arquivo\n",
    "    for e in exts:\n",
    "        # Busca todos os arquivos com a extensão atual e adiciona à lista (ordenados)\n",
    "        image_paths.extend(sorted(folder.glob(e)))\n",
    "    # Verifica se alguma imagem foi encontrada\n",
    "    if not image_paths:\n",
    "        # Lança erro se nenhuma imagem for encontrada\n",
    "        raise ValueError(f\"No images found in {folder}\")\n",
    "    # Limita o número de imagens ao máximo especificado\n",
    "    image_paths = image_paths[:max_images]\n",
    "    # Itera sobre cada caminho de imagem\n",
    "    for img_path in image_paths:\n",
    "        # Carrega a imagem usando OpenCV\n",
    "        img = cv2.imread(str(img_path))\n",
    "        # Verifica se a imagem foi carregada com sucesso\n",
    "        if img is None:\n",
    "            # Exibe mensagem de erro e pula para a próxima imagem\n",
    "            print(f\"Failed to load image: {img_path}\")\n",
    "            continue\n",
    "        # Obtém altura e largura da imagem\n",
    "        h, w = img.shape[:2]\n",
    "        # Obtém o caminho do arquivo .txt correspondente (mesmo nome, extensão .txt)\n",
    "        txt_path = img_path.with_suffix(\".txt\")\n",
    "        # Faz o parse do arquivo YOLO para obter as bounding boxes\n",
    "        boxes = parse_yolo_txt(txt_path, w, h)\n",
    "        # Cria uma cópia da imagem para o overlay (camada de cor transparente)\n",
    "        overlay = img.copy()\n",
    "        # Inicializa dicionário para armazenar cores por classe\n",
    "        colors = {}\n",
    "        # Itera sobre cada bounding box detectada\n",
    "        for box in boxes:\n",
    "            # Desempacota os valores da box (classe e coordenadas)\n",
    "            cls, x1, y1, x2, y2 = box\n",
    "            # Verifica se já existe uma cor definida para essa classe\n",
    "            if cls not in colors:\n",
    "                # Cria um gerador de números aleatórios com seed baseado na classe\n",
    "                rnd = random.Random(cls)\n",
    "                # Gera uma cor RGB aleatória (valores entre 50 e 255)\n",
    "                colors[cls] = (int(rnd.randint(50,255)), int(rnd.randint(50,255)), int(rnd.randint(50,255)))\n",
    "            # Obtém a cor para a classe atual\n",
    "            color = colors[cls]\n",
    "            # Desenha um retângulo preenchido no overlay com a cor da classe\n",
    "            cv2.rectangle(overlay, (x1,y1), (x2,y2), color, thickness=-1)\n",
    "            # Desenha a borda preta do retângulo na imagem original\n",
    "            cv2.rectangle(img, (x1,y1), (x2,y2), (0,0,0), thickness=2)\n",
    "            # Adiciona o texto com o ID da classe no canto superior esquerdo da box\n",
    "            cv2.putText(img, str(cls), (x1+3, y1+18), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2, cv2.LINE_AA)\n",
    "        # Mistura o overlay colorido com a imagem original usando o fator alpha\n",
    "        blended = cv2.addWeighted(overlay, alpha, img, 1-alpha, 0)\n",
    "        # Converte a imagem de BGR (OpenCV) para RGB (matplotlib)\n",
    "        blended_rgb = cv2.cvtColor(blended, cv2.COLOR_BGR2RGB)\n",
    "        # Verifica se deve mostrar os nomes dos arquivos\n",
    "        if show_filenames:\n",
    "            # Exibe o nome do arquivo e número de boxes em formato Markdown\n",
    "            display(Markdown(f\"**Preview:** `{img_path.name}`  —  {len(boxes)} box(es)\"))\n",
    "        # Cria uma nova figura com o tamanho especificado\n",
    "        plt.figure(figsize=figsize)\n",
    "        # Remove os eixos da visualização\n",
    "        plt.axis('off')\n",
    "        # Exibe a imagem mesclada\n",
    "        plt.imshow(blended_rgb)\n",
    "        # Renderiza a visualização\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870286d8-be4f-472e-8c00-6ba6cb9390be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378aeaa-b7a9-4bf6-bdb1-325ce5f71cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn on tfloat32 for Ampere GPUs\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# use bfloat16 for the entire notebook\n",
    "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed3380-e909-4cce-bbaa-b5cd2dba3728",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### construir o modelo\n",
    "[visite esta pagina para requisitar acesso pelo Hugging Face](https://huggingface.co/facebook/sam3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f7c32-78d1-48cf-b540-9cc66042225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abrir token para a API da plataforma Hugging Face\n",
    "with open('../hf_token.txt', 'r') as file:\n",
    "    my_token = file.read()\n",
    "\n",
    "#login\n",
    "login(token=my_token)\n",
    "\n",
    "#instância do modelo\n",
    "bpe_path = f\"./sam3/assets/bpe_simple_vocab_16e6.txt.gz\"\n",
    "model = build_sam3_image_model(bpe_path=bpe_path)\n",
    "processor = Sam3Processor(model, confidence_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8255ba-ff33-48c6-9f0e-1ef6147fab6b",
   "metadata": {},
   "source": [
    "###### inferencia\n",
    "esse processo foi realizado varias vezes por pasta de imagens da categoria, onde selecionamos a melhor palavra chave para o modelo encontrar o salgado nas imagens. Uma boa palavra para sanduiches não mapeia para coxinhas. \"Food\" e \"Snack\" foram os termos mais amplos identificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76d4e9-4b2f-465f-bf74-70cbd31c9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#selecionar classe para realizar o labeling em batelada\n",
    "folder = \"./dataset_sinth_cut/sanduiche\"\n",
    "\n",
    "salgados_e_classes={\"Bolinha de queijo\":0,\"Canapé\":1,\"Canudo\":2,\"Coxinha\":3,\"Croquete\":4,\"Empadinha\":5,\"Enroladinho de salsicha\":6,\"Esfiha\":7,\"Folhado\":8,\"Pastelzinho\":9,\"Pão de queijo\":10,\"Quibe\":11,\"Risoles\":12,\"Sanduiche\":13}\n",
    "\n",
    "#selecionar o salgado da pasta, o dicionário vai mapear para o encoding YOLO\n",
    "set_class = salgados_e_classes[\"Sanduiche\"]\n",
    "\n",
    "# valid image extensions\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".webp\"}\n",
    "\n",
    "#habilita uma segunda tentativa com um termo mais amplo caso o modelo não crie nenhum label com o prompt principal\n",
    "second_try=True\n",
    "\n",
    "#file iterator\n",
    "for root, _, files in os.walk(folder):\n",
    "    for f in files:\n",
    "        if os.path.splitext(f)[1].lower() in exts:\n",
    "            path = os.path.join(root, f)\n",
    "            total = SAM3_to_YOLO(path, \n",
    "                                 processor, \n",
    "                                 txt_prompt='sandwich', #main prompt\n",
    "                                 yolo_class = set_class)\n",
    "            if total == 0 and second_try==True:\n",
    "                print('0 ->', end = ' ')\n",
    "                total = SAM3_to_YOLO(path, \n",
    "                                 processor, \n",
    "                                 txt_prompt='hors d’oeuvres', #fallback prompt\n",
    "                                 yolo_class = set_class)\n",
    "            print(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3918bd9-e306-4bb7-8f09-a10bdf0291d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the labels\n",
    "folder = \"./dataset_sinth_cut/sanduiche\"\n",
    "preview_yolo_folder(folder, max_images=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a23d3-8547-4023-8abf-4b2663d65841",
   "metadata": {},
   "source": [
    "### Refinamento de labels com Label Studio\n",
    "Com um labeling bruto acurado na criação de *bounding boxes* para cada salgado do nosso *dataset*, mas com classes aplicadas de maneira grosseira pela categoria que encontramos a imagem fonte quando a pesquisamos no google, surge a demanda de usar uma ferramenta de uso ergonómico de gestão de datasets para corrigir as labels incorretas dentro de nossos dados. \n",
    "\n",
    "O [Label Studio](https://labelstud.io/) é um software de criação de *datasets* multimodal, selecionado por ser open source e apoiar o *upload* de dados pré-etiquetados. \n",
    "\n",
    "Reorganizamos nosso dataset YOLO na estrutura:\n",
    "\n",
    "```\n",
    "dataset_sinth_labeled_yolo_raw/\n",
    "├── images/\n",
    "│   ├── img_0001.jpg\n",
    "│   ├── img_0002.jpg\n",
    "│   ├── img_0003.jpg\n",
    "│   └── ...\n",
    "├── labels/\n",
    "│   ├── img_0001.txt \n",
    "│   ├── img_0002.txt\n",
    "│   ├── img_0003.txt\n",
    "│   └── ...\n",
    "└── classes.txt\n",
    "```\n",
    "\n",
    "E importamos os dados para o Label Studio usando o procedimento detalhado neste artigo: [Tutorial: Importing Local YOLO Pre-Annotated Images to Label Studio](https://labelstud.io/blog/tutorial-importing-local-yolo-pre-annotated-images-to-label-studio/).\n",
    "\n",
    "Dentro do *software*, procuramos imagens que requerem revisão nas labels da etapa passada e usamos a interfaçe intuitiva para fazer as correções. O conjunto de imagens 'group', de salgadinhos diversos, foi etiquetado pelo Sam3 como 'Coxinhas' na etapa passada e precisou de uma revisão aprofundada. \n",
    "\n",
    "![Imagem revisada no software Label Studio](relatorio2Images/uso_label_studio.png)\n",
    "Acima: Imagem revisada no software Label Studio.\n",
    "\n",
    "Com a revisão realizada, exportamos o dataset no formato COCO com Imagens.\n",
    "\n",
    "Para realizar o treino com o modelo RF-DETR, precisamos formatar o dataset em subconjuntos de treino, teste e validação. Como cada classe tem aproximadamente 30 imagens fonte baixadas da internet, optamos por fazer uma separação:\n",
    "- 70% Treino\n",
    "- 15% Validação\n",
    "- 15% Teste\n",
    "Com o conhecimento prévio de que as imagens geradas pelo Nano Banana podem ser trivialmente similares, a separação é consciente da imagem fonte. Por exemplo: As imagens coxinha_0012_3.jpg e coxinha_0012_1.jpg são abstrações da mesma imagem base coxinha_0012.jpg, então não podemos colocar uma no teste e outra na validação.\n",
    "\n",
    "O script de separação e formatação abaixo leva isso em conta. Também formatamos o Script para gerar um dataset monoclasse/monolabel, que treina um modelo que conta e identifica salgadinhos em uma imagem idependente do tipo de salgadinho que for. Ele tem uma unica classe, denominada 'Salgadinho'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e5340f-5688-45ba-a62d-fe411c624587",
   "metadata": {},
   "source": [
    "#### Refinamento de labels com Label Studio: Conversão para dataset COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c79d29-1526-41b4-9ef0-bc081dac2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Flag de configuração para converter todas as classes em uma única classe 'Salgadinho'\n",
    "monoclass = False  # Defina como True para converter todas as classes para uma única classe 'Salgadinho'\n",
    "\n",
    "# Carrega o dataset original em formato COCO\n",
    "path = \"synth_dataset_coco_labels.json\"\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Extrai as seções principais do arquivo COCO\n",
    "images = coco.get(\"images\", [])\n",
    "annotations = coco.get(\"annotations\", [])\n",
    "categories = coco.get(\"categories\", [])\n",
    "\n",
    "# Passo 1: Agrupa imagens pelo nome base (removendo sufixo de augmentação)\n",
    "def get_base_name(filename):\n",
    "    \"\"\"Extrai o nome base do arquivo, tratando sufixos de augmentação\"\"\"\n",
    "    # Obtém apenas o nome do arquivo, sem o caminho\n",
    "    basename = os.path.basename(filename)\n",
    "    # Divide do final para lidar com nomes como pao_de_queijo_0012_0.jpg\n",
    "    parts = basename.rsplit('_', 2)\n",
    "    \n",
    "    # Verifica se a última parte antes da extensão é um dígito (número de augmentação)\n",
    "    name_without_ext = basename.rsplit('.', 1)[0]\n",
    "    parts = name_without_ext.split('_')\n",
    "    \n",
    "    # Se a última parte é um dígito, remove ela (número de augmentação)\n",
    "    if parts[-1].isdigit():\n",
    "        # Verifica se a penúltima também é um dígito (o ID)\n",
    "        if len(parts) > 1 and parts[-2].isdigit():\n",
    "            # Retorna tudo exceto o último dígito\n",
    "            return '_'.join(parts[:-1])\n",
    "    \n",
    "    # Se nenhum sufixo de augmentação foi encontrado, retorna como está\n",
    "    return name_without_ext\n",
    "\n",
    "def get_class_name(filename):\n",
    "    \"\"\"Extrai o prefixo de classe/categoria do nome do arquivo\"\"\"\n",
    "    # Obtém apenas o nome do arquivo, sem o caminho\n",
    "    basename = os.path.basename(filename)\n",
    "    # Remove a extensão do arquivo\n",
    "    name_without_ext = basename.rsplit('.', 1)[0]\n",
    "    # Divide o nome em partes separadas por underscore\n",
    "    parts = name_without_ext.split('_')\n",
    "    \n",
    "    # Encontra onde o ID numérico começa (do final para o início)\n",
    "    # Mantém tudo antes das partes numéricas\n",
    "    for i in range(len(parts) - 1, -1, -1):\n",
    "        if not parts[i].isdigit():\n",
    "            return '_'.join(parts[:i+1])\n",
    "    \n",
    "    # Se não encontrou partes não-numéricas, retorna a primeira parte\n",
    "    return parts[0]\n",
    "\n",
    "# Agrupa imagens pelo seu nome base\n",
    "image_groups = defaultdict(list)\n",
    "for img in images:\n",
    "    # Obtém o nome base da imagem\n",
    "    base = get_base_name(img['file_name'])\n",
    "    # Adiciona a imagem ao grupo correspondente\n",
    "    image_groups[base].append(img)\n",
    "\n",
    "# Passo 2: Agrupa por prefixo de classe para dividi-las\n",
    "class_groups = defaultdict(list)\n",
    "for base_name in image_groups.keys():\n",
    "    # Obtém a classe a partir do nome base\n",
    "    class_name = get_class_name(base_name)\n",
    "    # Adiciona o nome base ao grupo da classe\n",
    "    class_groups[class_name].append(base_name)\n",
    "\n",
    "# Passo 3: Divide cada classe: primeiros 20 -> train, próximos 5 -> val, resto -> test\n",
    "split_assignment = {}  # Dicionário: base_name -> 'train'/'val'/'test'\n",
    "\n",
    "for class_name, base_names in class_groups.items():\n",
    "    # Embaralha a ordem para adicionar aleatoriedade!\n",
    "    shuffled_bases = base_names.copy()\n",
    "    random.shuffle(shuffled_bases)\n",
    "    \n",
    "    # Atribui cada imagem a um split baseado no índice\n",
    "    for idx, base in enumerate(shuffled_bases):\n",
    "        if idx < 20:\n",
    "            # Primeiros 20 vão para treino\n",
    "            split_assignment[base] = 'train'\n",
    "        elif idx < 25:\n",
    "            # Próximos 5 (índices 20-24) vão para validação\n",
    "            split_assignment[base] = 'val'\n",
    "        else:\n",
    "            # Restantes vão para teste\n",
    "            split_assignment[base] = 'test'\n",
    "\n",
    "# Passo 4: Atribui imagens aos splits baseado no seu grupo\n",
    "splits = {'train': [], 'val': [], 'test': []}\n",
    "for base_name, images_in_group in image_groups.items():\n",
    "    # Obtém o split atribuído a este grupo base\n",
    "    split = split_assignment[base_name]\n",
    "    # Adiciona todas as imagens do grupo ao split correspondente\n",
    "    splits[split].extend(images_in_group)\n",
    "\n",
    "# Passo 5: Cria a estrutura de diretórios de saída\n",
    "output_dir = Path('super_dataset')\n",
    "for split in ['train', 'val', 'test']:\n",
    "    # Cria diretório para cada split (train, val, test)\n",
    "    (output_dir / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Passo 6: Copia imagens e cria anotações para cada split\n",
    "source_img_dir = Path(\"./dataset_new_folderized1/images\")\n",
    "\n",
    "# Informações metadata do dataset\n",
    "info = {\n",
    "    \"description\": \"Salgados Dataset\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"year\": 2025\n",
    "}\n",
    "\n",
    "# Atualiza categorias com supercategoria\n",
    "updated_categories = []\n",
    "for cat in categories:\n",
    "    # Cria uma cópia da categoria\n",
    "    cat_copy = cat.copy()\n",
    "    # Se não tiver supercategoria, adiciona 'food'\n",
    "    if 'supercategory' not in cat_copy:\n",
    "        cat_copy['supercategory'] = 'food'\n",
    "    updated_categories.append(cat_copy)\n",
    "\n",
    "# Processa cada split (train, val, test)\n",
    "for split_name, split_images in splits.items():\n",
    "    print(f\"\\nProcessando split {split_name} com {len(split_images)} imagens...\")\n",
    "    \n",
    "    # Obtém os IDs de imagem deste split\n",
    "    split_image_ids = {img['id'] for img in split_images}\n",
    "    \n",
    "    # Filtra anotações para este split (apenas anotações das imagens deste split)\n",
    "    split_annotations = [ann for ann in annotations if ann['image_id'] in split_image_ids]\n",
    "    \n",
    "    # Trata a conversão para monoclass se habilitada\n",
    "    if monoclass:\n",
    "        # Muda todos os category_id nas anotações para 0\n",
    "        split_annotations = [\n",
    "            {**ann, 'category_id': 0} for ann in split_annotations\n",
    "        ]\n",
    "        # Cria uma única categoria\n",
    "        final_categories = [\n",
    "            {\n",
    "                \"id\": 0,\n",
    "                \"name\": \"Salgadinho\",\n",
    "                \"supercategory\": \"food\"\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        # Usa as categorias originais atualizadas\n",
    "        final_categories = updated_categories\n",
    "    \n",
    "    # Copia imagens e atualiza o campo file_name\n",
    "    updated_images = []\n",
    "    for img in split_images:\n",
    "        # Cria uma cópia da informação da imagem\n",
    "        img_copy = img.copy()\n",
    "        \n",
    "        # Extrai apenas o nome do arquivo\n",
    "        original_path = Path(img['file_name'])\n",
    "        filename = original_path.name\n",
    "        # Atualiza o file_name para ser apenas o nome do arquivo\n",
    "        img_copy['file_name'] = filename\n",
    "        \n",
    "        # Copia o arquivo de imagem - agora para o nível superior da pasta do split\n",
    "        source_path = source_img_dir / filename\n",
    "        dest_path = output_dir / split_name / filename\n",
    "        \n",
    "        # Verifica se o arquivo de origem existe antes de copiar\n",
    "        if source_path.exists():\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "        else:\n",
    "            print(f\"Aviso: {source_path} não encontrado\")\n",
    "        \n",
    "        # Adiciona a imagem atualizada à lista\n",
    "        updated_images.append(img_copy)\n",
    "    \n",
    "    # Cria o JSON no formato COCO para este split\n",
    "    split_coco = {\n",
    "        \"info\": info,\n",
    "        \"images\": updated_images,\n",
    "        \"annotations\": split_annotations,\n",
    "        \"categories\": final_categories\n",
    "    }\n",
    "    \n",
    "    # Salva o arquivo JSON com o nome correto\n",
    "    json_path = output_dir / split_name / '_annotations.coco.json'\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(split_coco, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Imprime estatísticas deste split\n",
    "    print(f\"{split_name}: {len(updated_images)} imagens, {len(split_annotations)} anotações\")\n",
    "\n",
    "# Imprime resumo final\n",
    "print(\"\\nDivisão do dataset completa!\")\n",
    "print(f\"Diretório de saída: {output_dir.absolute()}\")\n",
    "print(\"\\nResumo da divisão:\")\n",
    "print(f\"  Train: {len(splits['train'])} imagens\")\n",
    "print(f\"  Val: {len(splits['val'])} imagens\")\n",
    "print(f\"  Test: {len(splits['test'])} imagens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3971523-e392-4356-adce-45ee9a72e0e0",
   "metadata": {},
   "source": [
    "## Treinamento com RF-DETR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d162181-0ae9-4b32-b82a-781ff5f4611d",
   "metadata": {},
   "source": [
    "Para nosso modelo detector de objetos, buscamos atender aos seguintes critérios:\n",
    "- Performante em sistemas com poucos recursos computacionais, refletindo um desejo de embarcar nossa solução em um sistema que pode operar *offline* em qualquer padaria ou instalação;\n",
    "- Implementação de facil operação dentro do ambiente python, criando um material de facil operação para futuros estudantes;\n",
    "- *Open-source* sem limitações para aplicações comerciais futuras.\n",
    "\n",
    "Nossa restrição open-source limita o uso dos modelos mais modernos da familia YOLO, que são distribuidos pela empresa Ultralytic com uma licensa AGPL-3.0. Nossa busca nos levou ao modelo [RF-DETR](https://github.com/roboflow/rf-detr), que é distribuida sobre uma licensa Apache-2.0 e atende aos nossos demais critérios, estando no estado na arte em classificação de objetos em tempo real.\n",
    "\n",
    "![Comparação de métricas mAP no *dataset* COCO](relatorio2Images/RFDETR_COCO_map.png)\n",
    "Acima: Comparação de métricas mAP no *dataset* COCO. Fonte: [Roboflow](https://github.com/roboflow/rf-detr?tab=readme-ov-file)\n",
    "\n",
    "Com o objetivo de focar a avaliação de performance nos modelos mais 'portateis', reportamos nossas observações em cima do modelo 'Small' da família RD-DETR. Vamos treinar um modelo com nosso dataset multilabel, onde cada classe de salgadinho tem sua categoria individual, e o dataset monolabel, onde todos os salgadinhos pertencem a 'macro-classe' Salgadinho, e o objetivo é contar salgadinhos idependente da classe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535496b-31d2-47aa-8ac8-39700f9c46e0",
   "metadata": {},
   "source": [
    "### Treinamento com RF-DETR: Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b60e7-bd81-4329-9f8e-9d0842ecd9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "First_Time=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d10a1-6497-40ad-862d-e7d72d12714b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Instalar bibiliotecas usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181820d1-e616-47f9-8d14-41f05f5d500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if First_Time==True:\n",
    "    !pip install -q rfdetr==1.2.1 supervision==0.26.1 kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f4ea2-1819-4c5d-9442-1f5fe039f82c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Imports e definições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f53430-caa4-48ff-a937-0e5d9fb78036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from natsort import natsorted\n",
    "import shutil\n",
    "import gc\n",
    "import torch\n",
    "import weakref\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import supervision as sv\n",
    "from math import ceil\n",
    "from IPython.display import display\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "def zip_folder(folder_path, zip_path):\n",
    "    # Cria um arquivo ZIP em modo de escrita com compressão DEFLATED\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n",
    "        # Percorre todos os diretórios e arquivos dentro da pasta\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            # Para cada arquivo encontrado\n",
    "            for f in files:\n",
    "                # Obtém o caminho completo do arquivo\n",
    "                full = os.path.join(root, f)\n",
    "                # Calcula o caminho relativo ao folder_path\n",
    "                rel = os.path.relpath(full, folder_path)\n",
    "                # Escreve o arquivo no ZIP com o caminho relativo\n",
    "                z.write(full, rel)\n",
    "\n",
    "def move_and_rename_folder(src_folder, dest_folder, new_name):\n",
    "    \"\"\"\n",
    "    Move uma pasta localizada no diretório de trabalho atual para dest_folder,\n",
    "    renomeando-a para new_name no processo.\n",
    "    \"\"\"\n",
    "    # caminhos absolutos\n",
    "    # Obtém o diretório de trabalho atual\n",
    "    cwd = os.getcwd()\n",
    "    # Cria o caminho completo da pasta de origem\n",
    "    src_path = os.path.join(cwd, src_folder)\n",
    "    # Cria o caminho completo da pasta de destino com o novo nome\n",
    "    final_path = os.path.join(dest_folder, new_name)\n",
    "    \n",
    "    # garante que o diretório de destino existe\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    \n",
    "    # move (e sobrescreve se já existir)\n",
    "    # Se o destino final já existe\n",
    "    if os.path.exists(final_path):\n",
    "        # Remove a pasta existente completamente\n",
    "        shutil.rmtree(final_path)\n",
    "    # Move a pasta de origem para o destino\n",
    "    shutil.move(src_path, final_path)\n",
    "    # Retorna o caminho final da pasta movida\n",
    "    return final_path\n",
    "\n",
    "def cleanup_gpu_memory(obj=None, verbose: bool = False):\n",
    "    # Verifica se CUDA está disponível\n",
    "    if not torch.cuda.is_available():\n",
    "        # Se verbose estiver ativado, imprime mensagem informativa\n",
    "        if verbose:\n",
    "            print(\"[INFO] CUDA is not available. No GPU cleanup needed.\")\n",
    "            print(\"[INFO] CUDA não está disponível. Limpeza de GPU não é necessária.\")\n",
    "        return\n",
    "    \n",
    "    # Função interna para obter estatísticas de memória\n",
    "    def get_memory_stats():\n",
    "        # Obtém memória alocada na GPU\n",
    "        allocated = torch.cuda.memory_allocated()\n",
    "        # Obtém memória reservada na GPU\n",
    "        reserved = torch.cuda.memory_reserved()\n",
    "        # Retorna ambos os valores\n",
    "        return allocated, reserved\n",
    "    \n",
    "    # Sincroniza todas as operações CUDA pendentes\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Se verbose estiver ativado, mostra estatísticas antes da limpeza\n",
    "    if verbose:\n",
    "        alloc, reserv = get_memory_stats()\n",
    "        print(f\"[Before] Allocated: {alloc / 1024**2:.2f} MB | Reserved: {reserv / 1024**2:.2f} MB\")\n",
    "        print(f\"[Antes] Alocado: {alloc / 1024**2:.2f} MB | Reservado: {reserv / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Garante que removemos todas as referências fortes\n",
    "    # Se um objeto foi passado\n",
    "    if obj is not None:\n",
    "        # Cria uma referência fraca ao objeto\n",
    "        ref = weakref.ref(obj)\n",
    "        # Deleta a referência forte ao objeto\n",
    "        del obj\n",
    "        # Se o objeto ainda existe e verbose está ativo, imprime aviso\n",
    "        if ref() is not None and verbose:\n",
    "            print(\"[WARNING] Object not fully garbage collected yet.\")\n",
    "            print(\"[AVISO] Objeto ainda não foi completamente coletado pelo garbage collector.\")\n",
    "    \n",
    "    # Força a coleta de lixo do Python\n",
    "    gc.collect()\n",
    "    # Limpa o cache de memória da GPU\n",
    "    torch.cuda.empty_cache()\n",
    "    # Coleta objetos IPC (Inter-Process Communication) do CUDA\n",
    "    torch.cuda.ipc_collect()\n",
    "    # Sincroniza novamente todas as operações CUDA\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Se verbose estiver ativado, mostra estatísticas depois da limpeza\n",
    "    if verbose:\n",
    "        alloc, reserv = get_memory_stats()\n",
    "        print(f\"[After]  Allocated: {alloc / 1024**2:.2f} MB | Reserved: {reserv / 1024**2:.2f} MB\")\n",
    "        print(f\"[Depois]  Alocado: {alloc / 1024**2:.2f} MB | Reservado: {reserv / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3cb6a-bc27-4626-9fa7-c892b3d8164e",
   "metadata": {},
   "source": [
    "#### Descompactar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c32ca-539f-420f-8e7d-f7a55aa8834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if First_Time=True:\n",
    "    # Autenticar\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    \n",
    "    # Download dataset\n",
    "    api.dataset_download_files(\n",
    "        'vicenteborges/brazilian-salgadinho-object-detection',\n",
    "        path='.',  # onde fazer o download\n",
    "        unzip=True      # descompactar automáticamente\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8c489-c479-48f1-88b0-4c026ac7a11c",
   "metadata": {},
   "source": [
    "#### Configuração de modelos para treinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d125a480-8ea9-422d-9065-123529cfffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"nano\": \"RFDETRNano\",\n",
    "    \"small\": \"RFDETRSmall\",\n",
    "    \"medium\": \"RFDETRMedium\",\n",
    "    \"base\": \"RFDETRBase\",\n",
    "}\n",
    "\n",
    "\n",
    "datasets = {'multilabel': 'salgadosSinthCOCOsplitRFDETR',\n",
    "            'monolabel' : 'salgadosSinthCOCOsplitRFDETRmonolabel'}\n",
    "\n",
    "\n",
    "experiments=[]\n",
    "\n",
    "\n",
    "#for model in ['nano', 'small', 'medium', 'base']:\n",
    "\n",
    "#com nosso foco no modelo 'small', iteramos apenas por ele\n",
    "for model in ['small']:\n",
    "    for dataset in ['monolabel', 'multilabel']:\n",
    "        exp_dict={}\n",
    "        exp_dict['name']=f'RFDETR{model}_{dataset}'\n",
    "        exp_dict['model']=model\n",
    "        exp_dict['dataset']=dataset\n",
    "        exp_dict['epochs']=15\n",
    "        experiments.append(exp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef2fb5d-b034-4891-bc59-4ce47eeaebc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'RFDETRsmall_monolabel',\n",
       "  'model': 'small',\n",
       "  'dataset': 'monolabel',\n",
       "  'epochs': 15},\n",
       " {'name': 'RFDETRsmall_multilabel',\n",
       "  'model': 'small',\n",
       "  'dataset': 'multilabel',\n",
       "  'epochs': 15}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760dcaed-fd69-4b04-99c0-0cbb7edcad69",
   "metadata": {},
   "source": [
    "#### Treine o RF-DETR em um conjunto de dados personalizado\n",
    "> Escolha o `batch_size` adequado\n",
    "\n",
    "GPUs diferentes têm quantidades diferentes de VRAM (memória de vídeo), o que limita a quantidade de dados que podem processar simultaneamente durante o treinamento. Para fazer o treinamento funcionar bem em qualquer máquina, você pode ajustar duas configurações: `batch_size` e `grad_accum_steps`. Elas controlam quantas amostras são processadas por vez. O importante é manter o produto delas igual a 16 — esse é o tamanho total de lote que recomendamos. Por exemplo, em GPUs potentes como a A100, defina `batch_size=16` e `grad_accum_steps=1`. Em GPUs menores como a T4, use `batch_size=4` e `grad_accum_steps=4`. Utilizamos um método chamado acumulação de gradiente, que permite ao modelo simular o treinamento com um tamanho de lote maior ao coletar gradualmente as atualizações antes de ajustar os pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc3c497-d570-43a7-8cc9-bdc2bd57da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiments[:]:\n",
    "    model_name = models[experiment[\"model\"]]\n",
    "   \n",
    "    model_load_string1 = f\"from rfdetr import {model_name}\"\n",
    "\n",
    "    model_load_string2 = f\"{model_name}()\"\n",
    "\n",
    "    exec(model_load_string1)\n",
    "\n",
    "    model = eval(model_load_string2)\n",
    "    \n",
    "    model_dataset = datasets[experiment[\"dataset\"]]\n",
    "    print(\"###############\")\n",
    "    print(model_dataset)\n",
    "    print(\"###############\")\n",
    "    model.train(dataset_dir=model_dataset, \n",
    "                epochs=experiment[\"epochs\"], \n",
    "                batch_size=8, \n",
    "                grad_accum_steps=2)\n",
    "\n",
    "    move_and_rename_folder('output', 'outputs', experiment[\"name\"])\n",
    "\n",
    "    cleanup_gpu_memory(model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f782ac-3b0e-4488-ba31-40853e911b6e",
   "metadata": {},
   "source": [
    "#### Visualize todos os dados do treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b30ed-061c-4e1f-b22a-c204c95376e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import math\n",
    "\n",
    "def make_grid(base_dir=\"outputs\", output_name=\"combined.png\", cols=2, border=10):\n",
    "    # Inicializa lista vazia para armazenar os itens (nome da subpasta e caminho da imagem)\n",
    "    items = []\n",
    "    \n",
    "    # Coleta as imagens\n",
    "    # Percorre todos os subdiretórios dentro do diretório base, de forma ordenada\n",
    "    for sub in sorted(os.listdir(base_dir)):\n",
    "        # Constrói o caminho completo para o subdiretório\n",
    "        sub_path = os.path.join(base_dir, sub)\n",
    "        # Constrói o caminho completo para o arquivo metrics_plot.png dentro do subdiretório\n",
    "        img_path = os.path.join(sub_path, \"metrics_plot.png\")\n",
    "        # Verifica se sub_path é um diretório E se o arquivo metrics_plot.png existe\n",
    "        if os.path.isdir(sub_path) and os.path.isfile(img_path):\n",
    "            # Adiciona uma tupla (nome_da_pasta, caminho_da_imagem) na lista items\n",
    "            items.append((sub, img_path))\n",
    "    \n",
    "    # Se nenhuma imagem foi encontrada, imprime mensagem e encerra a função\n",
    "    if not items:\n",
    "        print(\"No metrics_plot.png found.\")\n",
    "        return\n",
    "    \n",
    "    # Define uma fonte maior (3× maior que o padrão típico ~12px)\n",
    "    try:\n",
    "        # Tenta carregar a fonte Arial TrueType com tamanho 36px para o texto vermelho\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 36)\n",
    "    except:\n",
    "        # Se não conseguir carregar a fonte TTF, usa a fonte padrão como fallback\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Carrega e rotula as imagens\n",
    "    # Inicializa lista vazia para armazenar as imagens já rotuladas\n",
    "    labeled_imgs = []\n",
    "    # Para cada par (nome, caminho) na lista de itens\n",
    "    for name, path in items:\n",
    "        # Abre a imagem e converte para modo RGBA (com canal alpha/transparência)\n",
    "        img = Image.open(path).convert(\"RGBA\")\n",
    "        # Cria um objeto de desenho para adicionar texto na imagem\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        # Desenha o nome da pasta na posição (10, 10) com cor vermelha e a fonte definida\n",
    "        draw.text((10, 10), name, fill=\"red\", font=font)\n",
    "        # Adiciona a imagem rotulada na lista\n",
    "        labeled_imgs.append(img)\n",
    "    \n",
    "    # Calcula o tamanho da grade\n",
    "    # Obtém largura e altura da primeira imagem (assume que todas têm o mesmo tamanho)\n",
    "    w, h = labeled_imgs[0].size\n",
    "    # Calcula o número de linhas necessárias: divide total de imagens por colunas e arredonda para cima\n",
    "    rows = math.ceil(len(labeled_imgs) / cols)\n",
    "    \n",
    "    # Inclui as bordas no tamanho do canvas final\n",
    "    # Largura final: (colunas × largura_imagem) + ((colunas + 1) × borda)\n",
    "    final_w = cols * w + (cols + 1) * border\n",
    "    # Altura final: (linhas × altura_imagem) + ((linhas + 1) × borda)\n",
    "    final_h = rows * h + (rows + 1) * border\n",
    "    # Cria uma nova imagem RGB com o tamanho calculado e fundo preto\n",
    "    final = Image.new(\"RGB\", (final_w, final_h), color=\"black\")\n",
    "    \n",
    "    # Cola as imagens com bordas\n",
    "    # Para cada imagem rotulada e seu índice\n",
    "    for i, img in enumerate(labeled_imgs):\n",
    "        # Calcula a linha: índice dividido por número de colunas (divisão inteira)\n",
    "        r = i // cols\n",
    "        # Calcula a coluna: resto da divisão do índice por número de colunas\n",
    "        c = i % cols\n",
    "        # Calcula posição X: borda inicial + coluna × (largura + borda)\n",
    "        x = border + c * (w + border)\n",
    "        # Calcula posição Y: borda inicial + linha × (altura + borda)\n",
    "        y = border + r * (h + border)\n",
    "        # Cola a imagem na posição calculada (x, y) no canvas final\n",
    "        final.paste(img, (x, y))\n",
    "    \n",
    "    # Salva a imagem final com o nome especificado\n",
    "    final.save(output_name)\n",
    "    # Imprime mensagem de confirmação\n",
    "    print(\"Saved as\", output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ba4c0-7ab9-4b22-9649-3bad3adb891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd47f78-b785-4e39-a46c-72373eee70c8",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "![Resultados de treinos no modelo RF-DETR Small para o *dataset* monolabel e multilabel](relatorio2Images/train_results.png)\n",
    "Acima: Resultados de treinos no modelo RF-DETR Small para o *dataset* monolabel e multilabel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a56c7470-108f-4d40-b927-e11d996db589",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Modelo RF-DETR_Small_Monolabel\n",
    "\n",
    "O modelo RF-DETR foi treinado para detecção monolabel de salgadinhos, tratando todas as variedades (coxinha, canapé, etc.) como uma única classe. A Tabela 1 apresenta as métricas obtidas nos conjuntos de validação e teste.\n",
    "\n",
    "**Tabela 1 - Métricas de desempenho do modelo RF-DETR**\n",
    "\n",
    "| Conjunto | mAP@50:95 | mAP@50 | Precision | Recall |\n",
    "|----------|-----------|---------|-----------|---------|\n",
    "| Validação | 94,66% | 98,55% | 98,00% | 97,00% |\n",
    "| Teste | 95,29% | 98,64% | 97,45% | 97,00% |\n",
    "\n",
    "#### Análise das Métricas: Monolabel\n",
    "\n",
    "O modelo apresentou métricas elevadas em todos os indicadores avaliados. O mAP@50:95 de 94,66% (validação) e 95,29% (teste) indica boa precisão na localização espacial dos objetos segundo os critérios de IoU estabelecidos. A precisão de 98,00% (validação) e recall de 97,00% sugerem baixa taxa de falsos positivos e cobertura adequada dos objetos presentes nas imagens de teste.\n",
    "\n",
    "### Modelo RF-DETR_Small_Monolabel\n",
    "\n",
    "O modelo RF-DETR foi treinado para detecção multilabel de salgadinhos, classificando 14 categorias distintas: Bolinha de queijo, Canapé, Canudo, Coxinha, Croquete, Empadinha, Enroladinho de salsicha, Esfiha, Folhado, Pastelzinho, Pão de queijo, Quibe, Risoles e Sanduíche. A Tabela 2 apresenta as métricas obtidas nos conjuntos de validação e teste para cada classe.\n",
    "\n",
    "**Tabela 2 - Métricas de desempenho por classe do modelo RF-DETR multilabel**\n",
    "\n",
    "| Classe | mAP@50:95 (Val) | mAP@50:95 (Test) | Precision (Val) | Precision (Test) | Recall |\n",
    "|--------|-----------------|------------------|-----------------|------------------|---------|\n",
    "| Bolinha de queijo | 75,52% | 86,90% | 54,46% | 85,54% | 89,00% |\n",
    "| Canapé | 94,18% | 93,93% | 92,86% | 95,10% | 89,00% |\n",
    "| Canudo | 89,91% | 95,30% | 94,78% | 99,35% | 89,00% |\n",
    "| Coxinha | 74,24% | 89,85% | 34,05% | 90,83% | 89,00% |\n",
    "| Croquete | 94,39% | 90,84% | 94,59% | 89,07% | 89,00% |\n",
    "| Empadinha | 93,93% | 96,29% | 97,93% | 100,00% | 89,00% |\n",
    "| Enroladinho de salsicha | 91,87% | 87,96% | 97,80% | 98,05% | 89,00% |\n",
    "| Esfiha | 94,57% | 98,85% | 99,22% | 99,53% | 89,00% |\n",
    "| Folhado | 97,69% | 92,49% | 100,00% | 91,04% | 89,00% |\n",
    "| Pastelzinho | 89,68% | 91,42% | 95,33% | 96,03% | 89,00% |\n",
    "| Pão de queijo | 96,18% | 95,09% | 98,45% | 100,00% | 89,00% |\n",
    "| Quibe | 91,01% | 62,63% | 88,11% | 51,04% | 89,00% |\n",
    "| Risoles | 99,41% | 98,11% | 100,00% | 98,11% | 89,00% |\n",
    "| Sanduíche | 74,31% | 87,43% | 78,11% | 88,66% | 89,00% |\n",
    "| **Média (all)** | **89,78%** | **90,51%** | **87,55%** | **91,60%** | **89,00%** |\n",
    "\n",
    "#### Análise das Métricas: Multilabel\n",
    "\n",
    "O modelo apresentou desempenho heterogêneo entre as diferentes classes de salgadinhos. A métrica média geral (mAP@50:95) foi de 89,78% no conjunto de validação e 90,51% no conjunto de teste, indicando capacidade adequada de detecção e localização espacial.\n",
    "\n",
    "**Classes com Alto Desempenho**: Risoles (99,41% val / 98,11% test), Folhado (97,69% val / 92,49% test), Pão de queijo (96,18% val / 95,09% test) e Esfiha (94,57% val / 98,85% test) demonstraram métricas consistentemente elevadas, com precision superior a 90% em ambos os conjuntos.\n",
    "\n",
    "**Classes com Desempenho Intermediário**: A maioria das classes (Canapé, Canudo, Croquete, Empadinha, Enroladinho de salsicha, Pastelzinho) apresentou mAP@50:95 entre 89% e 95%, com precision variando entre 87% e 99%, caracterizando desempenho satisfatório.\n",
    "\n",
    "**Classes com Desempenho Inferior**: Três classes apresentaram dificuldades significativas. Bolinha de queijo obteve apenas 54,46% de precision na validação (melhorando para 85,54% no teste), sugerindo alta taxa de falsos positivos durante o treinamento. Coxinha apresentou o pior desempenho na validação com 34,05% de precision, embora tenha melhorado substancialmente no teste (90,83%). Quibe mostrou degradação acentuada entre validação (91,01% mAP@50:95, 88,11% precision) e teste (62,63% mAP@50:95, 51,04% precision), indicando possível overfitting ou subrepresentação desta classe no conjunto de treinamento.\n",
    "\n",
    "**Recall Uniforme**: Todas as classes apresentaram recall fixo de 89,00%, o que sugere que o modelo foi avaliado com threshold de confiança uniforme para todas as categorias. Esta uniformidade pode não ser ideal para classes com características visuais distintas.\n",
    "\n",
    "**Variação entre Validação e Teste**: Observou-se variações consideráveis em algumas classes entre os conjuntos de validação e teste. Classes como Bolinha de queijo, Coxinha e Canudo apresentaram melhorias significativas no teste, enquanto Quibe e Folhado demonstraram degradação. Esta inconsistência sugere possível desbalanceamento nos conjuntos de dados ou variabilidade na representação das classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcde78b-8fda-4837-9d54-0e1f1860c274",
   "metadata": {},
   "source": [
    "## Discussão\n",
    "\n",
    "A análise conjunta dos gráficos de treinamento e das métricas finais evidencia diferenças claras entre o comportamento dos modelos RF-DETR nas tarefas monolabel e multilabel. No cenário monolabel, as curvas de loss mostram convergência rápida e estável, com o validation loss acompanhando de perto o training loss após as primeiras épocas. A suavidade dessas curvas e a ausência de oscilações significativas refletem a relativa simplicidade da tarefa: ao tratar todas as instâncias como uma única classe, o modelo não enfrenta competição entre categorias nem sobreposição semântica. Esse comportamento se alinha diretamente às métricas observadas, que apresentam mAP@50:95 superior a 94% e precisão/recall próximos ou acima de 97%. Os gráficos de AP corroboram essa estabilidade, com valores elevados e consistentes ao longo do treinamento, indicando que o modelo não apenas localiza bem os objetos, mas o faz de forma robusta a diferentes limiares de IoU.\n",
    "\n",
    "Em contraste, o modelo multilabel apresenta comportamento substancialmente mais complexo. Os gráficos de loss mostram curvas mais altas, com diferenças maiores entre treino e validação e oscilações persistentes, sugerindo maior variabilidade nos dados e dificuldade em generalizar uniformemente entre as 14 categorias. Embora o AP@50 cresça rapidamente — refletindo um aprendizado eficiente da tarefa de detecção — o AP@50:95 estabiliza-se em níveis mais baixos que no monolabel, consistente com o mAP médio de aproximadamente 90% observado nas métricas finais. Essa limitação se manifesta principalmente nas classes visualmente semelhantes, menos representadas ou com maior variação intra-classe, como Quibe, Coxinha e Bolinha de Queijo, cujos desempenhos inferiores ecoam as oscilações vistas nas curvas de AP durante o treinamento.\n",
    "\n",
    "Por fim, o padrão de recall praticamente constante nas curvas sugere impacto direto da estratégia de threshold utilizada na inferência, o que explica a uniformidade desse valor nas métricas por classe. Enquanto o modelo monolabel atinge desempenho elevado devido à simplicidade da tarefa, o modelo multilabel revela maior heterogeneidade, tanto no processo de treinamento quanto nos resultados por classe, refletindo os desafios inerentes à classificação fina de múltiplas categorias visualmente próximas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73385c-1de2-4a53-a1c2-fa0a09e0102f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## *Deploy* do modelo: código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb6dd2-20d8-4ffe-b74a-fc3d034ffe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from rfdetr import RFDETRSmall\n",
    "\n",
    "# ========== CONFIGURAÇÃO ==========\n",
    "MODE = \"video\"  # Opções: \"video\", \"webcam\"\n",
    "MODEL_PATH = \"RFDETRSmall_Multi_Sinth.pth\"  # Caminho específico do arquivo .pth\n",
    "CLASS_DICT = \"multilabel\"  # Opções: \"monolabel\", \"multilabel\"\n",
    "\n",
    "# Caminhos de Entrada/Saída (para modo video)\n",
    "INPUT_VIDEO = \"salgadinos2.mp4\"\n",
    "OUTPUT_VIDEO = \"salgadinos2_multi_sinth.mp4\"\n",
    "\n",
    "# Configurações da Webcam\n",
    "WEBCAM_ID = 0  # Webcam padrão\n",
    "\n",
    "# Limiar de detecção\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# ========== DICIONÁRIOS DE CLASSES ==========\n",
    "CLASS_DICTS = {\n",
    "    \"monolabel\": {\n",
    "        0: \"Salgado\"\n",
    "    },\n",
    "    \"multilabel\": {\n",
    "        0: \"Bolinha de queijo\",\n",
    "        1: \"Canapé\",\n",
    "        2: \"Canudo\",\n",
    "        3: \"Coxinha\",\n",
    "        4: \"Croquete\",\n",
    "        5: \"Empadinha\",\n",
    "        6: \"Enroladinho de salsicha\",\n",
    "        7: \"Esfiha\",\n",
    "        8: \"Folhado\",\n",
    "        9: \"Pastelzinho\",\n",
    "        10: \"Pão de queijo\",\n",
    "        11: \"Quibe\",\n",
    "        12: \"Risoles\",\n",
    "        13: \"Sanduiche\",\n",
    "    }\n",
    "}\n",
    "\n",
    "COCO_CLASSES = CLASS_DICTS[CLASS_DICT]\n",
    "\n",
    "# ========== CARREGAR MODELO ==========\n",
    "print(f\"Loading model from: {MODEL_PATH}\")  # Carregando modelo de: {caminho}\n",
    "model = RFDETRSmall(pretrain_weights=MODEL_PATH)  # Instancia o modelo com os pesos pré-treinados\n",
    "model.optimize_for_inference()  # Otimiza o modelo para inferência (execução mais rápida)\n",
    "print(\"Model loaded and optimized\")  # Modelo carregado e otimizado\n",
    "\n",
    "# ========== CONFIGURAR ANOTADORES ==========\n",
    "# Define uma paleta de cores em hexadecimal para as caixas delimitadoras\n",
    "color = sv.ColorPalette.from_hex([\n",
    "    \"#ffff00\", \"#ff9b00\", \"#ff8080\", \"#ff66b2\", \"#ff66ff\", \"#b266ff\",\n",
    "    \"#9999ff\", \"#3399ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
    "])\n",
    "\n",
    "def annotate_frame(frame, detections, counts_text):\n",
    "    \"\"\"Anota um único frame com detecções e contagens\"\"\"\n",
    "    # Converte BGR (OpenCV) para RGB (PIL)\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Calcula a escala de texto ideal baseada na resolução da imagem\n",
    "    text_scale = sv.calculate_optimal_text_scale(resolution_wh=image.size)\n",
    "    # Calcula a espessura de linha ideal baseada na resolução da imagem\n",
    "    thickness = sv.calculate_optimal_line_thickness(resolution_wh=image.size)\n",
    "    \n",
    "    # Cria um anotador para desenhar caixas delimitadoras\n",
    "    bbox_annotator = sv.BoxAnnotator(color=color, thickness=thickness)\n",
    "    # Cria um anotador para desenhar rótulos (labels)\n",
    "    label_annotator = sv.LabelAnnotator(\n",
    "        color=color,\n",
    "        text_color=sv.Color.BLACK,\n",
    "        text_scale=text_scale,\n",
    "        smart_position=True  # Posiciona os rótulos de forma inteligente\n",
    "    )\n",
    "    \n",
    "    # Cria uma lista de rótulos com o nome da classe e a confiança\n",
    "    labels = [\n",
    "        f\"{COCO_CLASSES[class_id]} {confidence:.2f}\"\n",
    "        for class_id, confidence\n",
    "        in zip(detections.class_id, detections.confidence)\n",
    "    ]\n",
    "    \n",
    "    # Anota a imagem com as caixas delimitadoras\n",
    "    annotated_image = bbox_annotator.annotate(image, detections)\n",
    "    # Anota a imagem com os rótulos\n",
    "    annotated_image = label_annotator.annotate(annotated_image, detections, labels)\n",
    "    \n",
    "    # Adiciona overlay com as contagens\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "    try:\n",
    "        # Tenta carregar uma fonte TrueType específica\n",
    "        font = ImageFont.truetype(\"VeraMono.ttf\", 40)\n",
    "    except:\n",
    "        # Se falhar, usa a fonte padrão\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Desenha o texto de contagens no canto superior esquerdo (posição 10, 10)\n",
    "    draw.multiline_text((10, 10), counts_text, fill=\"red\", font=font)\n",
    "    \n",
    "    # Converte de volta para BGR (OpenCV)\n",
    "    return cv2.cvtColor(np.array(annotated_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def process_frame(frame):\n",
    "    \"\"\"Processa um único frame e retorna o frame anotado\"\"\"\n",
    "    # Converte para imagem PIL\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Executa a detecção\n",
    "    detections = model.predict(image, threshold=THRESHOLD)\n",
    "    \n",
    "    # Conta as classes detectadas\n",
    "    counts = {cid: 0 for cid in COCO_CLASSES.keys()}  # Inicializa contador para cada classe\n",
    "    for cid in detections.class_id:\n",
    "        counts[int(cid)] += 1  # Incrementa o contador da classe detectada\n",
    "    \n",
    "    # Constrói o texto com as contagens\n",
    "    lines = []\n",
    "    for cid in sorted(counts.keys()):\n",
    "        if counts[cid] > 0:  # Mostra apenas classes que foram detectadas\n",
    "            lines.append(f\"{COCO_CLASSES[cid]}: {counts[cid]}\")\n",
    "    counts_text = \"\\n\".join(lines) if lines else \"No detections\"  # Se não houver detecções, mostra \"No detections\"\n",
    "    \n",
    "    # Anota o frame\n",
    "    return annotate_frame(frame, detections, counts_text)\n",
    "\n",
    "# ========== MODO VÍDEO ==========\n",
    "if MODE == \"video\":\n",
    "    print(f\"Processing video: {INPUT_VIDEO}\")  # Processando vídeo: {caminho}\n",
    "    \n",
    "    # Abre o arquivo de vídeo\n",
    "    cap = cv2.VideoCapture(INPUT_VIDEO)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Cannot open video: {INPUT_VIDEO}\")  # Não foi possível abrir o vídeo\n",
    "    \n",
    "    # Obtém as propriedades do vídeo\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # Frames por segundo\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Largura\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Altura\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Total de frames\n",
    "    \n",
    "    # Configura o escritor de vídeo para salvar o resultado\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec MP4\n",
    "    out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Video info: {width}x{height} @ {fps}fps, {total_frames} frames\")  # Informações do vídeo\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        # Lê o próximo frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Sai do loop se não houver mais frames\n",
    "        \n",
    "        frame_count += 1\n",
    "        print(f\"Processing frame {frame_count}/{total_frames}\", end='\\r')  # Mostra progresso\n",
    "        \n",
    "        # Processa e anota o frame\n",
    "        annotated_frame = process_frame(frame)\n",
    "        # Escreve o frame anotado no vídeo de saída\n",
    "        out.write(annotated_frame)\n",
    "    \n",
    "    # Libera os recursos\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"\\nVideo saved to: {OUTPUT_VIDEO}\")  # Vídeo salvo em: {caminho}\n",
    "\n",
    "# ========== MODO WEBCAM ==========\n",
    "elif MODE == \"webcam\":\n",
    "    print(f\"Starting webcam (ID: {WEBCAM_ID}). Press 'q' to quit.\")  # Iniciando webcam. Pressione 'q' para sair.\n",
    "    \n",
    "    # Abre a webcam\n",
    "    cap = cv2.VideoCapture(WEBCAM_ID)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Cannot open webcam: {WEBCAM_ID}\")  # Não foi possível abrir a webcam\n",
    "    \n",
    "    while True:\n",
    "        # Captura o frame da webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")  # Falha ao capturar frame\n",
    "            break\n",
    "        \n",
    "        # Processa e anota o frame\n",
    "        annotated_frame = process_frame(frame)\n",
    "        \n",
    "        # Exibe o frame\n",
    "        cv2.imshow('RFDETR Webcam Detection', annotated_frame)\n",
    "        \n",
    "        # Pressione 'q' para sair\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Libera os recursos\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Webcam stream closed\")  # Stream da webcam fechado\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Invalid MODE: {MODE}. Use 'video' or 'webcam'\")  # MODO inválido. Use 'video' ou 'webcam'\n",
    "\n",
    "print(\"Done!\")  # Concluído!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
